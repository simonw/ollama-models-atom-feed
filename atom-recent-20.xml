<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Ollama models</title>
  <id>https://ollama.com/search?o=newest</id>
  <author>
    <name>Model Library</name>
  </author>
  <link href="https://ollama.com/search?o=newest" rel="self"/>
  <updated>2025-03-28T06:36:27.510719+00:00</updated>
  <entry>
    <title>gemma3</title>
    <id>https://ollama.com/library/gemma3</id>
    <link href="https://ollama.com/library/gemma3"/>
    <summary>The current, most capable model that runs on a single GPU.</summary>
    <updated>2025-03-25T00:12:00+00:00</updated>
    <category term="1b"/>
    <category term="4b"/>
    <category term="12b"/>
    <category term="27b"/>
    <category term="vision"/>
    <content type="html">&lt;p&gt;The current, most capable model that runs on a single GPU.&lt;/p&gt;&lt;p&gt;Pulls: 2.9M&lt;/p&gt;&lt;p&gt;Tags: 17&lt;/p&gt;</content>
  </entry>
  <entry>
    <title>exaone-deep</title>
    <id>https://ollama.com/library/exaone-deep</id>
    <link href="https://ollama.com/library/exaone-deep"/>
    <summary>EXAONE Deep exhibits superior capabilities in various reasoning tasks including math and coding benchmarks, ranging from 2.4B to 32B parameters developed and released by LG AI Research.</summary>
    <updated>2025-03-19T17:04:00+00:00</updated>
    <category term="2.4b"/>
    <category term="7.8b"/>
    <category term="32b"/>
    <content type="html">&lt;p&gt;EXAONE Deep exhibits superior capabilities in various reasoning tasks including math and coding benchmarks, ranging from 2.4B to 32B parameters developed and released by LG AI Research.&lt;/p&gt;&lt;p&gt;Pulls: 14K&lt;/p&gt;&lt;p&gt;Tags: 13&lt;/p&gt;</content>
  </entry>
  <entry>
    <title>qwq</title>
    <id>https://ollama.com/library/qwq</id>
    <link href="https://ollama.com/library/qwq"/>
    <summary>QwQ is the reasoning model of the Qwen series.</summary>
    <updated>2025-03-13T18:55:00+00:00</updated>
    <category term="32b"/>
    <category term="tools"/>
    <content type="html">&lt;p&gt;QwQ is the reasoning model of the Qwen series.&lt;/p&gt;&lt;p&gt;Pulls: 1.1M&lt;/p&gt;&lt;p&gt;Tags: 8&lt;/p&gt;</content>
  </entry>
  <entry>
    <title>command-a</title>
    <id>https://ollama.com/library/command-a</id>
    <link href="https://ollama.com/library/command-a"/>
    <summary>111 billion parameter model optimized for demanding enterprises that require fast, secure, and high-quality AI</summary>
    <updated>2025-03-13T12:39:00+00:00</updated>
    <category term="111b"/>
    <category term="tools"/>
    <content type="html">&lt;p&gt;111 billion parameter model optimized for demanding enterprises that require fast, secure, and high-quality AI&lt;/p&gt;&lt;p&gt;Pulls: 4,233&lt;/p&gt;&lt;p&gt;Tags: 5&lt;/p&gt;</content>
  </entry>
  <entry>
    <title>command-r7b-arabic</title>
    <id>https://ollama.com/library/command-r7b-arabic</id>
    <link href="https://ollama.com/library/command-r7b-arabic"/>
    <summary>A new state-of-the-art version of the lightweight Command R7B model that excels in advanced Arabic language capabilities for enterprises in the Middle East and Northern Africa.</summary>
    <updated>2025-02-28T21:28:00+00:00</updated>
    <category term="7b"/>
    <category term="tools"/>
    <content type="html">&lt;p&gt;A new state-of-the-art version of the lightweight Command R7B model that excels in advanced Arabic language capabilities for enterprises in the Middle East and Northern Africa.&lt;/p&gt;&lt;p&gt;Pulls: 4,127&lt;/p&gt;&lt;p&gt;Tags: 5&lt;/p&gt;</content>
  </entry>
  <entry>
    <title>phi4-mini</title>
    <id>https://ollama.com/library/phi4-mini</id>
    <link href="https://ollama.com/library/phi4-mini"/>
    <summary>Phi-4-mini brings significant enhancements in multilingual support, reasoning, and mathematics, and now, the long-awaited function calling feature is finally supported.</summary>
    <updated>2025-02-28T20:02:00+00:00</updated>
    <category term="3.8b"/>
    <category term="tools"/>
    <content type="html">&lt;p&gt;Phi-4-mini brings significant enhancements in multilingual support, reasoning, and mathematics, and now, the long-awaited function calling feature is finally supported.&lt;/p&gt;&lt;p&gt;Pulls: 84.5K&lt;/p&gt;&lt;p&gt;Tags: 5&lt;/p&gt;</content>
  </entry>
  <entry>
    <title>granite3.2-vision</title>
    <id>https://ollama.com/library/granite3.2-vision</id>
    <link href="https://ollama.com/library/granite3.2-vision"/>
    <summary>A compact and efficient vision-language model, specifically designed for visual document understanding, enabling automated content extraction from tables, charts, infographics, plots, diagrams, and more.</summary>
    <updated>2025-02-27T19:26:00+00:00</updated>
    <category term="2b"/>
    <category term="vision"/>
    <category term="tools"/>
    <content type="html">&lt;p&gt;A compact and efficient vision-language model, specifically designed for visual document understanding, enabling automated content extraction from tables, charts, infographics, plots, diagrams, and more.&lt;/p&gt;&lt;p&gt;Pulls: 30.6K&lt;/p&gt;&lt;p&gt;Tags: 5&lt;/p&gt;</content>
  </entry>
  <entry>
    <title>granite3.2</title>
    <id>https://ollama.com/library/granite3.2</id>
    <link href="https://ollama.com/library/granite3.2"/>
    <summary>Granite-3.2 is a family of long-context AI models from IBM Granite fine-tuned for thinking capabilities.</summary>
    <updated>2025-02-26T05:48:00+00:00</updated>
    <category term="2b"/>
    <category term="8b"/>
    <category term="tools"/>
    <content type="html">&lt;p&gt;Granite-3.2 is a family of long-context AI models from IBM Granite fine-tuned for thinking capabilities.&lt;/p&gt;&lt;p&gt;Pulls: 51.8K&lt;/p&gt;&lt;p&gt;Tags: 9&lt;/p&gt;</content>
  </entry>
  <entry>
    <title>r1-1776</title>
    <id>https://ollama.com/library/r1-1776</id>
    <link href="https://ollama.com/library/r1-1776"/>
    <summary>A version of the DeepSeek-R1 model that has been post trained to provide unbiased, accurate, and factual information by Perplexity.</summary>
    <updated>2025-02-21T15:28:00+00:00</updated>
    <category term="70b"/>
    <category term="671b"/>
    <content type="html">&lt;p&gt;A version of the DeepSeek-R1 model that has been post trained to provide unbiased, accurate, and factual information by Perplexity.&lt;/p&gt;&lt;p&gt;Pulls: 19.6K&lt;/p&gt;&lt;p&gt;Tags: 9&lt;/p&gt;</content>
  </entry>
  <entry>
    <title>openthinker</title>
    <id>https://ollama.com/library/openthinker</id>
    <link href="https://ollama.com/library/openthinker"/>
    <summary>A fully open-source family of reasoning models built using a dataset derived by distilling DeepSeek-R1.</summary>
    <updated>2025-02-12T19:26:00+00:00</updated>
    <category term="7b"/>
    <category term="32b"/>
    <content type="html">&lt;p&gt;A fully open-source family of reasoning models built using a dataset derived by distilling DeepSeek-R1.&lt;/p&gt;&lt;p&gt;Pulls: 514.7K&lt;/p&gt;&lt;p&gt;Tags: 9&lt;/p&gt;</content>
  </entry>
  <entry>
    <title>deepscaler</title>
    <id>https://ollama.com/library/deepscaler</id>
    <link href="https://ollama.com/library/deepscaler"/>
    <summary>A fine-tuned version of Deepseek-R1-Distilled-Qwen-1.5B that surpasses the performance of OpenAI‚Äôs o1-preview with just 1.5B parameters on popular math evaluations.</summary>
    <updated>2025-02-12T01:53:00+00:00</updated>
    <category term="1.5b"/>
    <content type="html">&lt;p&gt;A fine-tuned version of Deepseek-R1-Distilled-Qwen-1.5B that surpasses the performance of OpenAI‚Äôs o1-preview with just 1.5B parameters on popular math evaluations.&lt;/p&gt;&lt;p&gt;Pulls: 70.5K&lt;/p&gt;&lt;p&gt;Tags: 5&lt;/p&gt;</content>
  </entry>
  <entry>
    <title>deepseek-r1</title>
    <id>https://ollama.com/library/deepseek-r1</id>
    <link href="https://ollama.com/library/deepseek-r1"/>
    <summary>DeepSeek's first-generation of reasoning models with comparable performance to OpenAI-o1, including six dense models distilled from DeepSeek-R1 based on Llama and Qwen.</summary>
    <updated>2025-02-07T01:40:00+00:00</updated>
    <category term="1.5b"/>
    <category term="7b"/>
    <category term="8b"/>
    <category term="14b"/>
    <category term="32b"/>
    <category term="70b"/>
    <category term="671b"/>
    <content type="html">&lt;p&gt;DeepSeek's first-generation of reasoning models with comparable performance to OpenAI-o1, including six dense models distilled from DeepSeek-R1 based on Llama and Qwen.&lt;/p&gt;&lt;p&gt;Pulls: 30.6M&lt;/p&gt;&lt;p&gt;Tags: 29&lt;/p&gt;</content>
  </entry>
  <entry>
    <title>mistral-small</title>
    <id>https://ollama.com/library/mistral-small</id>
    <link href="https://ollama.com/library/mistral-small"/>
    <summary>Mistral Small 3 sets a new benchmark in the ‚Äúsmall‚Äù Large Language Models category below 70B.</summary>
    <updated>2025-01-30T15:58:00+00:00</updated>
    <category term="22b"/>
    <category term="24b"/>
    <category term="tools"/>
    <content type="html">&lt;p&gt;Mistral Small 3 sets a new benchmark in the ‚Äúsmall‚Äù Large Language Models category below 70B.&lt;/p&gt;&lt;p&gt;Pulls: 384.6K&lt;/p&gt;&lt;p&gt;Tags: 21&lt;/p&gt;</content>
  </entry>
  <entry>
    <title>granite3.1-dense</title>
    <id>https://ollama.com/library/granite3.1-dense</id>
    <link href="https://ollama.com/library/granite3.1-dense"/>
    <summary>The IBM Granite 2B and 8B models are text-only dense LLMs trained on over 12 trillion tokens of data, demonstrated significant improvements over their predecessors in performance and speed in IBM‚Äôs initial testing.</summary>
    <updated>2025-01-17T22:46:00+00:00</updated>
    <category term="2b"/>
    <category term="8b"/>
    <category term="tools"/>
    <content type="html">&lt;p&gt;The IBM Granite 2B and 8B models are text-only dense LLMs trained on over 12 trillion tokens of data, demonstrated significant improvements over their predecessors in performance and speed in IBM‚Äôs initial testing.&lt;/p&gt;&lt;p&gt;Pulls: 85.8K&lt;/p&gt;&lt;p&gt;Tags: 33&lt;/p&gt;</content>
  </entry>
  <entry>
    <title>granite3.1-moe</title>
    <id>https://ollama.com/library/granite3.1-moe</id>
    <link href="https://ollama.com/library/granite3.1-moe"/>
    <summary>The IBM Granite 1B and 3B models are long-context mixture of experts (MoE) Granite models from IBM designed for low latency usage.</summary>
    <updated>2025-01-17T22:18:00+00:00</updated>
    <category term="1b"/>
    <category term="3b"/>
    <category term="tools"/>
    <content type="html">&lt;p&gt;The IBM Granite 1B and 3B models are long-context mixture of experts (MoE) Granite models from IBM designed for low latency usage.&lt;/p&gt;&lt;p&gt;Pulls: 37.7K&lt;/p&gt;&lt;p&gt;Tags: 33&lt;/p&gt;</content>
  </entry>
  <entry>
    <title>command-r7b</title>
    <id>https://ollama.com/library/command-r7b</id>
    <link href="https://ollama.com/library/command-r7b"/>
    <summary>The smallest model in Cohere's R series delivers top-tier speed, efficiency, and quality to build powerful AI applications on commodity GPUs and edge devices.</summary>
    <updated>2025-01-16T22:27:00+00:00</updated>
    <category term="7b"/>
    <category term="tools"/>
    <content type="html">&lt;p&gt;The smallest model in Cohere's R series delivers top-tier speed, efficiency, and quality to build powerful AI applications on commodity GPUs and edge devices.&lt;/p&gt;&lt;p&gt;Pulls: 27.5K&lt;/p&gt;&lt;p&gt;Tags: 5&lt;/p&gt;</content>
  </entry>
  <entry>
    <title>deepseek-v3</title>
    <id>https://ollama.com/library/deepseek-v3</id>
    <link href="https://ollama.com/library/deepseek-v3"/>
    <summary>A strong Mixture-of-Experts (MoE) language model with 671B total parameters with 37B activated for each token.</summary>
    <updated>2025-01-13T17:16:00+00:00</updated>
    <category term="671b"/>
    <content type="html">&lt;p&gt;A strong Mixture-of-Experts (MoE) language model with 671B total parameters with 37B activated for each token.&lt;/p&gt;&lt;p&gt;Pulls: 886.7K&lt;/p&gt;&lt;p&gt;Tags: 5&lt;/p&gt;</content>
  </entry>
  <entry>
    <title>olmo2</title>
    <id>https://ollama.com/library/olmo2</id>
    <link href="https://ollama.com/library/olmo2"/>
    <summary>OLMo 2 is a new family of 7B and 13B models trained on up to 5T tokens. These models are on par with or better than equivalently sized fully open models, and competitive with open-weight models such as Llama 3.1 on English academic benchmarks.</summary>
    <updated>2025-01-11T22:43:00+00:00</updated>
    <category term="7b"/>
    <category term="13b"/>
    <content type="html">&lt;p&gt;OLMo 2 is a new family of 7B and 13B models trained on up to 5T tokens. These models are on par with or better than equivalently sized fully open models, and competitive with open-weight models such as Llama 3.1 on English academic benchmarks.&lt;/p&gt;&lt;p&gt;Pulls: 495.5K&lt;/p&gt;&lt;p&gt;Tags: 9&lt;/p&gt;</content>
  </entry>
  <entry>
    <title>phi4</title>
    <id>https://ollama.com/library/phi4</id>
    <link href="https://ollama.com/library/phi4"/>
    <summary>Phi-4 is a 14B parameter, state-of-the-art open model from Microsoft.</summary>
    <updated>2025-01-08T18:08:00+00:00</updated>
    <category term="14b"/>
    <content type="html">&lt;p&gt;Phi-4 is a 14B parameter, state-of-the-art open model from Microsoft.&lt;/p&gt;&lt;p&gt;Pulls: 1.2M&lt;/p&gt;&lt;p&gt;Tags: 5&lt;/p&gt;</content>
  </entry>
  <entry>
    <title>dolphin3</title>
    <id>https://ollama.com/library/dolphin3</id>
    <link href="https://ollama.com/library/dolphin3"/>
    <summary>Dolphin 3.0 Llama 3.1 8B üê¨ is the next generation of the Dolphin series of instruct-tuned models designed to be the ultimate general purpose local model, enabling coding, math, agentic, function calling, and general use cases.</summary>
    <updated>2025-01-05T19:38:00+00:00</updated>
    <category term="8b"/>
    <content type="html">&lt;p&gt;Dolphin 3.0 Llama 3.1 8B üê¨ is the next generation of the Dolphin series of instruct-tuned models designed to be the ultimate general purpose local model, enabling coding, math, agentic, function calling, and general use cases.&lt;/p&gt;&lt;p&gt;Pulls: 533.9K&lt;/p&gt;&lt;p&gt;Tags: 5&lt;/p&gt;</content>
  </entry>
</feed>
