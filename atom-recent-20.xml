<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Ollama models</title>
  <id>https://ollama.com/search?o=newest</id>
  <author>
    <name>Model Library</name>
  </author>
  <link href="https://ollama.com/search?o=newest" rel="self"/>
  <updated>2025-10-19T06:37:02.051134+00:00</updated>
  <entry>
    <title>glm-4.6</title>
    <id>https://ollama.com/library/glm-4.6</id>
    <link href="https://ollama.com/library/glm-4.6"/>
    <summary>Advanced agentic, reasoning and coding capabilities.</summary>
    <updated>2025-10-15T05:17:00+00:00</updated>
    <content type="html">&lt;p&gt;Advanced agentic, reasoning and coding capabilities.&lt;/p&gt;&lt;p&gt;Pulls: 4,410&lt;/p&gt;&lt;p&gt;Tags: 1&lt;/p&gt;</content>
  </entry>
  <entry>
    <title>qwen3-vl</title>
    <id>https://ollama.com/library/qwen3-vl</id>
    <link href="https://ollama.com/library/qwen3-vl"/>
    <summary>The most powerful vision-language model in the Qwen model family to date.</summary>
    <updated>2025-10-14T22:14:00+00:00</updated>
    <category term="vision"/>
    <content type="html">&lt;p&gt;The most powerful vision-language model in the Qwen model family to date.&lt;/p&gt;&lt;p&gt;Pulls: 6,567&lt;/p&gt;&lt;p&gt;Tags: 1&lt;/p&gt;</content>
  </entry>
  <entry>
    <title>qwen3</title>
    <id>https://ollama.com/library/qwen3</id>
    <link href="https://ollama.com/library/qwen3"/>
    <summary>Qwen3 is the latest generation of large language models in Qwen series, offering a comprehensive suite of dense and mixture-of-experts (MoE) models.</summary>
    <updated>2025-10-10T20:18:00+00:00</updated>
    <category term="0.6b"/>
    <category term="1.7b"/>
    <category term="4b"/>
    <category term="8b"/>
    <category term="14b"/>
    <category term="30b"/>
    <category term="32b"/>
    <category term="235b"/>
    <category term="tools"/>
    <category term="thinking"/>
    <content type="html">&lt;p&gt;Qwen3 is the latest generation of large language models in Qwen series, offering a comprehensive suite of dense and mixture-of-experts (MoE) models.&lt;/p&gt;&lt;p&gt;Pulls: 10.3M&lt;/p&gt;&lt;p&gt;Tags: 58&lt;/p&gt;</content>
  </entry>
  <entry>
    <title>gpt-oss</title>
    <id>https://ollama.com/library/gpt-oss</id>
    <link href="https://ollama.com/library/gpt-oss"/>
    <summary>OpenAI’s open-weight models designed for powerful reasoning, agentic tasks, and versatile developer use cases.</summary>
    <updated>2025-10-09T19:47:00+00:00</updated>
    <category term="20b"/>
    <category term="120b"/>
    <category term="tools"/>
    <category term="thinking"/>
    <content type="html">&lt;p&gt;OpenAI’s open-weight models designed for powerful reasoning, agentic tasks, and versatile developer use cases.&lt;/p&gt;&lt;p&gt;Pulls: 3.6M&lt;/p&gt;&lt;p&gt;Tags: 5&lt;/p&gt;</content>
  </entry>
  <entry>
    <title>granite4</title>
    <id>https://ollama.com/library/granite4</id>
    <link href="https://ollama.com/library/granite4"/>
    <summary>Granite 4 features improved instruction following (IF) and tool-calling capabilities, making them more effective in enterprise applications.</summary>
    <updated>2025-10-02T20:17:00+00:00</updated>
    <category term="tools"/>
    <content type="html">&lt;p&gt;Granite 4 features improved instruction following (IF) and tool-calling capabilities, making them more effective in enterprise applications.&lt;/p&gt;&lt;p&gt;Pulls: 40.5K&lt;/p&gt;&lt;p&gt;Tags: 5&lt;/p&gt;</content>
  </entry>
  <entry>
    <title>deepseek-v3.1</title>
    <id>https://ollama.com/library/deepseek-v3.1</id>
    <link href="https://ollama.com/library/deepseek-v3.1"/>
    <summary>DeepSeek-V3.1-Terminus is a hybrid model that supports both thinking mode and non-thinking mode.</summary>
    <updated>2025-09-27T00:46:00+00:00</updated>
    <category term="671b"/>
    <category term="tools"/>
    <category term="thinking"/>
    <content type="html">&lt;p&gt;DeepSeek-V3.1-Terminus is a hybrid model that supports both thinking mode and non-thinking mode.&lt;/p&gt;&lt;p&gt;Pulls: 105.8K&lt;/p&gt;&lt;p&gt;Tags: 8&lt;/p&gt;</content>
  </entry>
  <entry>
    <title>kimi-k2</title>
    <id>https://ollama.com/library/kimi-k2</id>
    <link href="https://ollama.com/library/kimi-k2"/>
    <summary>A state-of-the-art mixture-of-experts (MoE) language model. Kimi K2-Instruct-0905 demonstrates significant improvements in performance on public benchmarks and real-world coding agent tasks.</summary>
    <updated>2025-09-26T03:50:00+00:00</updated>
    <content type="html">&lt;p&gt;A state-of-the-art mixture-of-experts (MoE) language model. Kimi K2-Instruct-0905 demonstrates significant improvements in performance on public benchmarks and real-world coding agent tasks.&lt;/p&gt;&lt;p&gt;Pulls: 6,902&lt;/p&gt;&lt;p&gt;Tags: 1&lt;/p&gt;</content>
  </entry>
  <entry>
    <title>qwen3-embedding</title>
    <id>https://ollama.com/library/qwen3-embedding</id>
    <link href="https://ollama.com/library/qwen3-embedding"/>
    <summary>Building upon the foundational models of the Qwen3 series, Qwen3 Embedding provides a comprehensive range of text embeddings models in various sizes</summary>
    <updated>2025-09-23T20:26:00+00:00</updated>
    <category term="0.6b"/>
    <category term="4b"/>
    <category term="8b"/>
    <category term="embedding"/>
    <content type="html">&lt;p&gt;Building upon the foundational models of the Qwen3 series, Qwen3 Embedding provides a comprehensive range of text embeddings models in various sizes&lt;/p&gt;&lt;p&gt;Pulls: 68.7K&lt;/p&gt;&lt;p&gt;Tags: 12&lt;/p&gt;</content>
  </entry>
  <entry>
    <title>qwen3-coder</title>
    <id>https://ollama.com/library/qwen3-coder</id>
    <link href="https://ollama.com/library/qwen3-coder"/>
    <summary>Alibaba's performant long context models for agentic and coding tasks.</summary>
    <updated>2025-09-23T19:14:00+00:00</updated>
    <category term="30b"/>
    <category term="480b"/>
    <category term="tools"/>
    <content type="html">&lt;p&gt;Alibaba's performant long context models for agentic and coding tasks.&lt;/p&gt;&lt;p&gt;Pulls: 514.2K&lt;/p&gt;&lt;p&gt;Tags: 10&lt;/p&gt;</content>
  </entry>
  <entry>
    <title>embeddinggemma</title>
    <id>https://ollama.com/library/embeddinggemma</id>
    <link href="https://ollama.com/library/embeddinggemma"/>
    <summary>EmbeddingGemma is a 300M parameter embedding model from Google.</summary>
    <updated>2025-09-09T00:17:00+00:00</updated>
    <category term="300m"/>
    <category term="embedding"/>
    <content type="html">&lt;p&gt;EmbeddingGemma is a 300M parameter embedding model from Google.&lt;/p&gt;&lt;p&gt;Pulls: 93.6K&lt;/p&gt;&lt;p&gt;Tags: 5&lt;/p&gt;</content>
  </entry>
  <entry>
    <title>gemma3</title>
    <id>https://ollama.com/library/gemma3</id>
    <link href="https://ollama.com/library/gemma3"/>
    <summary>The current, most capable model that runs on a single GPU.</summary>
    <updated>2025-08-15T08:15:00+00:00</updated>
    <category term="270m"/>
    <category term="1b"/>
    <category term="4b"/>
    <category term="12b"/>
    <category term="27b"/>
    <category term="vision"/>
    <content type="html">&lt;p&gt;The current, most capable model that runs on a single GPU.&lt;/p&gt;&lt;p&gt;Pulls: 21.5M&lt;/p&gt;&lt;p&gt;Tags: 26&lt;/p&gt;</content>
  </entry>
  <entry>
    <title>mistral-nemo</title>
    <id>https://ollama.com/library/mistral-nemo</id>
    <link href="https://ollama.com/library/mistral-nemo"/>
    <summary>A state-of-the-art 12B model with 128k context length, built by Mistral AI in collaboration with NVIDIA.</summary>
    <updated>2025-07-23T18:43:00+00:00</updated>
    <category term="12b"/>
    <category term="tools"/>
    <content type="html">&lt;p&gt;A state-of-the-art 12B model with 128k context length, built by Mistral AI in collaboration with NVIDIA.&lt;/p&gt;&lt;p&gt;Pulls: 2.7M&lt;/p&gt;&lt;p&gt;Tags: 17&lt;/p&gt;</content>
  </entry>
  <entry>
    <title>mistral</title>
    <id>https://ollama.com/library/mistral</id>
    <link href="https://ollama.com/library/mistral"/>
    <summary>The 7B model released by Mistral AI, updated to version 0.3.</summary>
    <updated>2025-07-13T00:56:00+00:00</updated>
    <category term="7b"/>
    <category term="tools"/>
    <content type="html">&lt;p&gt;The 7B model released by Mistral AI, updated to version 0.3.&lt;/p&gt;&lt;p&gt;Pulls: 20.9M&lt;/p&gt;&lt;p&gt;Tags: 84&lt;/p&gt;</content>
  </entry>
  <entry>
    <title>devstral</title>
    <id>https://ollama.com/library/devstral</id>
    <link href="https://ollama.com/library/devstral"/>
    <summary>Devstral: the best open source model for coding agents</summary>
    <updated>2025-07-04T18:29:00+00:00</updated>
    <category term="24b"/>
    <category term="tools"/>
    <content type="html">&lt;p&gt;Devstral: the best open source model for coding agents&lt;/p&gt;&lt;p&gt;Pulls: 405.7K&lt;/p&gt;&lt;p&gt;Tags: 5&lt;/p&gt;</content>
  </entry>
  <entry>
    <title>deepseek-r1</title>
    <id>https://ollama.com/library/deepseek-r1</id>
    <link href="https://ollama.com/library/deepseek-r1"/>
    <summary>DeepSeek-R1 is a family of open reasoning models with performance approaching that of leading models, such as O3 and Gemini 2.5 Pro.</summary>
    <updated>2025-07-02T06:09:00+00:00</updated>
    <category term="1.5b"/>
    <category term="7b"/>
    <category term="8b"/>
    <category term="14b"/>
    <category term="32b"/>
    <category term="70b"/>
    <category term="671b"/>
    <category term="tools"/>
    <category term="thinking"/>
    <content type="html">&lt;p&gt;DeepSeek-R1 is a family of open reasoning models with performance approaching that of leading models, such as O3 and Gemini 2.5 Pro.&lt;/p&gt;&lt;p&gt;Pulls: 66.7M&lt;/p&gt;&lt;p&gt;Tags: 35&lt;/p&gt;</content>
  </entry>
  <entry>
    <title>gemma3n</title>
    <id>https://ollama.com/library/gemma3n</id>
    <link href="https://ollama.com/library/gemma3n"/>
    <summary>Gemma 3n models are designed for efficient execution on everyday devices such as laptops, tablets or phones.</summary>
    <updated>2025-06-27T05:02:00+00:00</updated>
    <category term="e2b"/>
    <category term="e4b"/>
    <content type="html">&lt;p&gt;Gemma 3n models are designed for efficient execution on everyday devices such as laptops, tablets or phones.&lt;/p&gt;&lt;p&gt;Pulls: 603.6K&lt;/p&gt;&lt;p&gt;Tags: 9&lt;/p&gt;</content>
  </entry>
  <entry>
    <title>mistral-small3.2</title>
    <id>https://ollama.com/library/mistral-small3.2</id>
    <link href="https://ollama.com/library/mistral-small3.2"/>
    <summary>An update to Mistral Small that improves on function calling, instruction following, and less repetition errors.</summary>
    <updated>2025-06-20T22:59:00+00:00</updated>
    <category term="24b"/>
    <category term="vision"/>
    <category term="tools"/>
    <content type="html">&lt;p&gt;An update to Mistral Small that improves on function calling, instruction following, and less repetition errors.&lt;/p&gt;&lt;p&gt;Pulls: 700K&lt;/p&gt;&lt;p&gt;Tags: 5&lt;/p&gt;</content>
  </entry>
  <entry>
    <title>magistral</title>
    <id>https://ollama.com/library/magistral</id>
    <link href="https://ollama.com/library/magistral"/>
    <summary>Magistral is a small, efficient reasoning model with 24B parameters.</summary>
    <updated>2025-06-16T20:46:00+00:00</updated>
    <category term="24b"/>
    <category term="tools"/>
    <category term="thinking"/>
    <content type="html">&lt;p&gt;Magistral is a small, efficient reasoning model with 24B parameters.&lt;/p&gt;&lt;p&gt;Pulls: 522.8K&lt;/p&gt;&lt;p&gt;Tags: 5&lt;/p&gt;</content>
  </entry>
  <entry>
    <title>llama4</title>
    <id>https://ollama.com/library/llama4</id>
    <link href="https://ollama.com/library/llama4"/>
    <summary>Meta's latest collection of multimodal models.</summary>
    <updated>2025-06-16T20:45:00+00:00</updated>
    <category term="16x17b"/>
    <category term="128x17b"/>
    <category term="vision"/>
    <category term="tools"/>
    <content type="html">&lt;p&gt;Meta's latest collection of multimodal models.&lt;/p&gt;&lt;p&gt;Pulls: 740.2K&lt;/p&gt;&lt;p&gt;Tags: 11&lt;/p&gt;</content>
  </entry>
  <entry>
    <title>qwen2.5-coder</title>
    <id>https://ollama.com/library/qwen2.5-coder</id>
    <link href="https://ollama.com/library/qwen2.5-coder"/>
    <summary>The latest series of Code-Specific Qwen models, with significant improvements in code generation, code reasoning, and code fixing.</summary>
    <updated>2025-05-28T01:19:00+00:00</updated>
    <category term="0.5b"/>
    <category term="1.5b"/>
    <category term="3b"/>
    <category term="7b"/>
    <category term="14b"/>
    <category term="32b"/>
    <category term="tools"/>
    <content type="html">&lt;p&gt;The latest series of Code-Specific Qwen models, with significant improvements in code generation, code reasoning, and code fixing.&lt;/p&gt;&lt;p&gt;Pulls: 7.7M&lt;/p&gt;&lt;p&gt;Tags: 199&lt;/p&gt;</content>
  </entry>
</feed>
