<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Ollama models</title>
  <id>https://ollama.com/search?o=newest</id>
  <author>
    <name>Model Library</name>
  </author>
  <link href="https://ollama.com/search?o=newest" rel="self"/>
  <updated>2025-12-23T06:39:37.040555+00:00</updated>
  <entry>
    <title>minimax-m2.1</title>
    <id>https://ollama.com/library/minimax-m2.1</id>
    <link href="https://ollama.com/library/minimax-m2.1"/>
    <summary>Exceptional multilingual capabilities to elevate code engineering</summary>
    <updated>2025-12-23T03:19:00+00:00</updated>
    <content type="html">&lt;p&gt;Exceptional multilingual capabilities to elevate code engineering&lt;/p&gt;&lt;p&gt;Pulls: 48&lt;/p&gt;&lt;p&gt;Tags: 1&lt;/p&gt;</content>
  </entry>
  <entry>
    <title>gemini-3-flash-preview</title>
    <id>https://ollama.com/library/gemini-3-flash-preview</id>
    <link href="https://ollama.com/library/gemini-3-flash-preview"/>
    <summary>Gemini 3 Flash offers frontier intelligence built for speed at a fraction of the cost.</summary>
    <updated>2025-12-20T20:44:00+00:00</updated>
    <content type="html">&lt;p&gt;Gemini 3 Flash offers frontier intelligence built for speed at a fraction of the cost.&lt;/p&gt;&lt;p&gt;Pulls: 6,491&lt;/p&gt;&lt;p&gt;Tags: 2&lt;/p&gt;</content>
  </entry>
  <entry>
    <title>deepseek-v3.2</title>
    <id>https://ollama.com/library/deepseek-v3.2</id>
    <link href="https://ollama.com/library/deepseek-v3.2"/>
    <summary>DeepSeek-V3.2, a model that harmonizes high computational efficiency with superior reasoning and agent performance.</summary>
    <updated>2025-12-19T18:09:00+00:00</updated>
    <content type="html">&lt;p&gt;DeepSeek-V3.2, a model that harmonizes high computational efficiency with superior reasoning and agent performance.&lt;/p&gt;&lt;p&gt;Pulls: 5,539&lt;/p&gt;&lt;p&gt;Tags: 1&lt;/p&gt;</content>
  </entry>
  <entry>
    <title>functiongemma</title>
    <id>https://ollama.com/library/functiongemma</id>
    <link href="https://ollama.com/library/functiongemma"/>
    <summary>FunctionGemma is a specialized version of Google's Gemma 3 270M model fine-tuned explicitly for function calling.</summary>
    <updated>2025-12-18T07:03:00+00:00</updated>
    <category term="270m"/>
    <content type="html">&lt;p&gt;FunctionGemma is a specialized version of Google's Gemma 3 270M model fine-tuned explicitly for function calling.&lt;/p&gt;&lt;p&gt;Pulls: 6,116&lt;/p&gt;&lt;p&gt;Tags: 4&lt;/p&gt;</content>
  </entry>
  <entry>
    <title>nemotron-3-nano</title>
    <id>https://ollama.com/library/nemotron-3-nano</id>
    <link href="https://ollama.com/library/nemotron-3-nano"/>
    <summary>Nemotron 3 Nano - A new Standard for Efficient, Open, and Intelligent Agentic Models</summary>
    <updated>2025-12-16T06:27:00+00:00</updated>
    <category term="30b"/>
    <content type="html">&lt;p&gt;Nemotron 3 Nano - A new Standard for Efficient, Open, and Intelligent Agentic Models&lt;/p&gt;&lt;p&gt;Pulls: 38.8K&lt;/p&gt;&lt;p&gt;Tags: 6&lt;/p&gt;</content>
  </entry>
  <entry>
    <title>olmo-3.1</title>
    <id>https://ollama.com/library/olmo-3.1</id>
    <link href="https://ollama.com/library/olmo-3.1"/>
    <summary>Olmo is a series of Open language models designed to enable the science of language models. These models are pre-trained on the Dolma 3 dataset and post-trained on the Dolci datasets.</summary>
    <updated>2025-12-16T05:56:00+00:00</updated>
    <category term="32b"/>
    <category term="tools"/>
    <content type="html">&lt;p&gt;Olmo is a series of Open language models designed to enable the science of language models. These models are pre-trained on the Dolma 3 dataset and post-trained on the Dolci datasets.&lt;/p&gt;&lt;p&gt;Pulls: 7,379&lt;/p&gt;&lt;p&gt;Tags: 10&lt;/p&gt;</content>
  </entry>
  <entry>
    <title>olmo-3</title>
    <id>https://ollama.com/library/olmo-3</id>
    <link href="https://ollama.com/library/olmo-3"/>
    <summary>Olmo is a series of Open language models designed to enable the science of language models. These models are pre-trained on the Dolma 3 dataset and post-trained on the Dolci datasets.</summary>
    <updated>2025-12-16T05:55:00+00:00</updated>
    <category term="7b"/>
    <category term="32b"/>
    <content type="html">&lt;p&gt;Olmo is a series of Open language models designed to enable the science of language models. These models are pre-trained on the Dolma 3 dataset and post-trained on the Dolci datasets.&lt;/p&gt;&lt;p&gt;Pulls: 10.2K&lt;/p&gt;&lt;p&gt;Tags: 15&lt;/p&gt;</content>
  </entry>
  <entry>
    <title>devstral-small-2</title>
    <id>https://ollama.com/library/devstral-small-2</id>
    <link href="https://ollama.com/library/devstral-small-2"/>
    <summary>24B model that excels at using tools to explore codebases, editing multiple files and power software engineering agents.</summary>
    <updated>2025-12-13T06:47:00+00:00</updated>
    <category term="24b"/>
    <category term="vision"/>
    <category term="tools"/>
    <content type="html">&lt;p&gt;24B model that excels at using tools to explore codebases, editing multiple files and power software engineering agents.&lt;/p&gt;&lt;p&gt;Pulls: 46.5K&lt;/p&gt;&lt;p&gt;Tags: 6&lt;/p&gt;</content>
  </entry>
  <entry>
    <title>ministral-3</title>
    <id>https://ollama.com/library/ministral-3</id>
    <link href="https://ollama.com/library/ministral-3"/>
    <summary>The Ministral 3 family is designed for edge deployment, capable of running on a wide range of hardware.</summary>
    <updated>2025-12-13T01:59:00+00:00</updated>
    <category term="3b"/>
    <category term="8b"/>
    <category term="14b"/>
    <category term="vision"/>
    <category term="tools"/>
    <content type="html">&lt;p&gt;The Ministral 3 family is designed for edge deployment, capable of running on a wide range of hardware.&lt;/p&gt;&lt;p&gt;Pulls: 124.6K&lt;/p&gt;&lt;p&gt;Tags: 16&lt;/p&gt;</content>
  </entry>
  <entry>
    <title>rnj-1</title>
    <id>https://ollama.com/library/rnj-1</id>
    <link href="https://ollama.com/library/rnj-1"/>
    <summary>Rnj-1 is a family of 8B parameter open-weight, dense models trained from scratch by Essential AI, optimized for code and STEM with capabilities on par with SOTA open-weight models.</summary>
    <updated>2025-12-12T23:44:00+00:00</updated>
    <category term="8b"/>
    <category term="tools"/>
    <content type="html">&lt;p&gt;Rnj-1 is a family of 8B parameter open-weight, dense models trained from scratch by Essential AI, optimized for code and STEM with capabilities on par with SOTA open-weight models.&lt;/p&gt;&lt;p&gt;Pulls: 12.3K&lt;/p&gt;&lt;p&gt;Tags: 6&lt;/p&gt;</content>
  </entry>
  <entry>
    <title>devstral-2</title>
    <id>https://ollama.com/library/devstral-2</id>
    <link href="https://ollama.com/library/devstral-2"/>
    <summary>123B model that excels at using tools to explore codebases, editing multiple files and power software engineering agents.</summary>
    <updated>2025-12-12T01:21:00+00:00</updated>
    <category term="123b"/>
    <category term="tools"/>
    <content type="html">&lt;p&gt;123B model that excels at using tools to explore codebases, editing multiple files and power software engineering agents.&lt;/p&gt;&lt;p&gt;Pulls: 12.4K&lt;/p&gt;&lt;p&gt;Tags: 6&lt;/p&gt;</content>
  </entry>
  <entry>
    <title>nomic-embed-text-v2-moe</title>
    <id>https://ollama.com/library/nomic-embed-text-v2-moe</id>
    <link href="https://ollama.com/library/nomic-embed-text-v2-moe"/>
    <summary>nomic-embed-text-v2-moe is a multilingual MoE text embedding model that excels at multilingual retrieval.</summary>
    <updated>2025-12-10T22:09:00+00:00</updated>
    <category term="embedding"/>
    <content type="html">&lt;p&gt;nomic-embed-text-v2-moe is a multilingual MoE text embedding model that excels at multilingual retrieval.&lt;/p&gt;&lt;p&gt;Pulls: 9,302&lt;/p&gt;&lt;p&gt;Tags: 1&lt;/p&gt;</content>
  </entry>
  <entry>
    <title>qwen3-next</title>
    <id>https://ollama.com/library/qwen3-next</id>
    <link href="https://ollama.com/library/qwen3-next"/>
    <summary>The first installment in the Qwen3-Next series with strong performance in terms of both parameter efficiency and inference speed.</summary>
    <updated>2025-12-09T00:41:00+00:00</updated>
    <category term="80b"/>
    <category term="tools"/>
    <category term="thinking"/>
    <content type="html">&lt;p&gt;The first installment in the Qwen3-Next series with strong performance in terms of both parameter efficiency and inference speed.&lt;/p&gt;&lt;p&gt;Pulls: 210.5K&lt;/p&gt;&lt;p&gt;Tags: 10&lt;/p&gt;</content>
  </entry>
  <entry>
    <title>gemma3</title>
    <id>https://ollama.com/library/gemma3</id>
    <link href="https://ollama.com/library/gemma3"/>
    <summary>The current, most capable model that runs on a single GPU.</summary>
    <updated>2025-12-05T21:46:00+00:00</updated>
    <category term="270m"/>
    <category term="1b"/>
    <category term="4b"/>
    <category term="12b"/>
    <category term="27b"/>
    <category term="vision"/>
    <content type="html">&lt;p&gt;The current, most capable model that runs on a single GPU.&lt;/p&gt;&lt;p&gt;Pulls: 28.6M&lt;/p&gt;&lt;p&gt;Tags: 29&lt;/p&gt;</content>
  </entry>
  <entry>
    <title>mistral-large-3</title>
    <id>https://ollama.com/library/mistral-large-3</id>
    <link href="https://ollama.com/library/mistral-large-3"/>
    <summary>A general-purpose multimodal mixture-of-experts model for production-grade tasks and enterprise workloads.</summary>
    <updated>2025-12-02T23:19:00+00:00</updated>
    <content type="html">&lt;p&gt;A general-purpose multimodal mixture-of-experts model for production-grade tasks and enterprise workloads.&lt;/p&gt;&lt;p&gt;Pulls: 5,207&lt;/p&gt;&lt;p&gt;Tags: 1&lt;/p&gt;</content>
  </entry>
  <entry>
    <title>cogito-2.1</title>
    <id>https://ollama.com/library/cogito-2.1</id>
    <link href="https://ollama.com/library/cogito-2.1"/>
    <summary>The Cogito v2.1 LLMs are instruction tuned generative models. All models are released under MIT license for commercial use.</summary>
    <updated>2025-11-21T20:18:00+00:00</updated>
    <category term="671b"/>
    <content type="html">&lt;p&gt;The Cogito v2.1 LLMs are instruction tuned generative models. All models are released under MIT license for commercial use.&lt;/p&gt;&lt;p&gt;Pulls: 20.2K&lt;/p&gt;&lt;p&gt;Tags: 6&lt;/p&gt;</content>
  </entry>
  <entry>
    <title>deepseek-ocr</title>
    <id>https://ollama.com/library/deepseek-ocr</id>
    <link href="https://ollama.com/library/deepseek-ocr"/>
    <summary>DeepSeek-OCR is a vision-language model that can perform token-efficient OCR.</summary>
    <updated>2025-11-19T20:26:00+00:00</updated>
    <category term="3b"/>
    <category term="vision"/>
    <content type="html">&lt;p&gt;DeepSeek-OCR is a vision-language model that can perform token-efficient OCR.&lt;/p&gt;&lt;p&gt;Pulls: 70K&lt;/p&gt;&lt;p&gt;Tags: 3&lt;/p&gt;</content>
  </entry>
  <entry>
    <title>gemini-3-pro-preview</title>
    <id>https://ollama.com/library/gemini-3-pro-preview</id>
    <link href="https://ollama.com/library/gemini-3-pro-preview"/>
    <summary>Google's most intelligent model with SOTA reasoning and multimodal understanding, and powerful agentic and vibe coding capabilities.</summary>
    <updated>2025-11-18T17:06:00+00:00</updated>
    <content type="html">&lt;p&gt;Google's most intelligent model with SOTA reasoning and multimodal understanding, and powerful agentic and vibe coding capabilities.&lt;/p&gt;&lt;p&gt;Pulls: 40.9K&lt;/p&gt;&lt;p&gt;Tags: 1&lt;/p&gt;</content>
  </entry>
  <entry>
    <title>kimi-k2-thinking</title>
    <id>https://ollama.com/library/kimi-k2-thinking</id>
    <link href="https://ollama.com/library/kimi-k2-thinking"/>
    <summary>Kimi K2 Thinking, Moonshot AI's best open-source thinking model.</summary>
    <updated>2025-11-07T01:16:00+00:00</updated>
    <content type="html">&lt;p&gt;Kimi K2 Thinking, Moonshot AI's best open-source thinking model.&lt;/p&gt;&lt;p&gt;Pulls: 14.8K&lt;/p&gt;&lt;p&gt;Tags: 1&lt;/p&gt;</content>
  </entry>
  <entry>
    <title>qwen3-vl</title>
    <id>https://ollama.com/library/qwen3-vl</id>
    <link href="https://ollama.com/library/qwen3-vl"/>
    <summary>The most powerful vision-language model in the Qwen model family to date.</summary>
    <updated>2025-10-29T23:24:00+00:00</updated>
    <category term="2b"/>
    <category term="4b"/>
    <category term="8b"/>
    <category term="30b"/>
    <category term="32b"/>
    <category term="235b"/>
    <category term="vision"/>
    <category term="tools"/>
    <content type="html">&lt;p&gt;The most powerful vision-language model in the Qwen model family to date.&lt;/p&gt;&lt;p&gt;Pulls: 821.9K&lt;/p&gt;&lt;p&gt;Tags: 59&lt;/p&gt;</content>
  </entry>
</feed>
