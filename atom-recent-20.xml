<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Ollama models</title>
  <id>https://ollama.com/search?o=newest</id>
  <author>
    <name>Model Library</name>
  </author>
  <link href="https://ollama.com/search?o=newest" rel="self"/>
  <updated>2025-04-08T06:37:03.736799+00:00</updated>
  <entry>
    <title>cogito</title>
    <id>https://ollama.com/library/cogito</id>
    <link href="https://ollama.com/library/cogito"/>
    <summary>Cogito v1 Preview is a family of hybrid reasoning models by Deep Cogito that outperform the best available open models of the same size, including counterparts from LLaMA, DeepSeek, and Qwen across most standard benchmarks.</summary>
    <updated>2025-04-08T01:45:00+00:00</updated>
    <category term="3b"/>
    <category term="8b"/>
    <category term="14b"/>
    <category term="32b"/>
    <category term="70b"/>
    <category term="tools"/>
    <content type="html">&lt;p&gt;Cogito v1 Preview is a family of hybrid reasoning models by Deep Cogito that outperform the best available open models of the same size, including counterparts from LLaMA, DeepSeek, and Qwen across most standard benchmarks.&lt;/p&gt;&lt;p&gt;Pulls: 694&lt;/p&gt;&lt;p&gt;Tags: 20&lt;/p&gt;</content>
  </entry>
  <entry>
    <title>mistral-small3.1</title>
    <id>https://ollama.com/library/mistral-small3.1</id>
    <link href="https://ollama.com/library/mistral-small3.1"/>
    <summary>Building upon Mistral Small 3, Mistral Small 3.1 (2503) adds state-of-the-art vision understanding and enhances long context capabilities up to 128k tokens without compromising text performance.</summary>
    <updated>2025-04-07T16:59:00+00:00</updated>
    <category term="24b"/>
    <category term="vision"/>
    <category term="tools"/>
    <content type="html">&lt;p&gt;Building upon Mistral Small 3, Mistral Small 3.1 (2503) adds state-of-the-art vision understanding and enhances long context capabilities up to 128k tokens without compromising text performance.&lt;/p&gt;&lt;p&gt;Pulls: 3,239&lt;/p&gt;&lt;p&gt;Tags: 5&lt;/p&gt;</content>
  </entry>
  <entry>
    <title>openthinker</title>
    <id>https://ollama.com/library/openthinker</id>
    <link href="https://ollama.com/library/openthinker"/>
    <summary>A fully open-source family of reasoning models built using a dataset derived by distilling DeepSeek-R1.</summary>
    <updated>2025-04-04T21:52:00+00:00</updated>
    <category term="7b"/>
    <category term="32b"/>
    <content type="html">&lt;p&gt;A fully open-source family of reasoning models built using a dataset derived by distilling DeepSeek-R1.&lt;/p&gt;&lt;p&gt;Pulls: 517.5K&lt;/p&gt;&lt;p&gt;Tags: 15&lt;/p&gt;</content>
  </entry>
  <entry>
    <title>gemma3</title>
    <id>https://ollama.com/library/gemma3</id>
    <link href="https://ollama.com/library/gemma3"/>
    <summary>The current, most capable model that runs on a single GPU.</summary>
    <updated>2025-03-25T00:12:00+00:00</updated>
    <category term="1b"/>
    <category term="4b"/>
    <category term="12b"/>
    <category term="27b"/>
    <category term="vision"/>
    <content type="html">&lt;p&gt;The current, most capable model that runs on a single GPU.&lt;/p&gt;&lt;p&gt;Pulls: 3.2M&lt;/p&gt;&lt;p&gt;Tags: 17&lt;/p&gt;</content>
  </entry>
  <entry>
    <title>exaone-deep</title>
    <id>https://ollama.com/library/exaone-deep</id>
    <link href="https://ollama.com/library/exaone-deep"/>
    <summary>EXAONE Deep exhibits superior capabilities in various reasoning tasks including math and coding benchmarks, ranging from 2.4B to 32B parameters developed and released by LG AI Research.</summary>
    <updated>2025-03-19T17:04:00+00:00</updated>
    <category term="2.4b"/>
    <category term="7.8b"/>
    <category term="32b"/>
    <content type="html">&lt;p&gt;EXAONE Deep exhibits superior capabilities in various reasoning tasks including math and coding benchmarks, ranging from 2.4B to 32B parameters developed and released by LG AI Research.&lt;/p&gt;&lt;p&gt;Pulls: 25K&lt;/p&gt;&lt;p&gt;Tags: 13&lt;/p&gt;</content>
  </entry>
  <entry>
    <title>qwq</title>
    <id>https://ollama.com/library/qwq</id>
    <link href="https://ollama.com/library/qwq"/>
    <summary>QwQ is the reasoning model of the Qwen series.</summary>
    <updated>2025-03-13T18:55:00+00:00</updated>
    <category term="32b"/>
    <category term="tools"/>
    <content type="html">&lt;p&gt;QwQ is the reasoning model of the Qwen series.&lt;/p&gt;&lt;p&gt;Pulls: 1.2M&lt;/p&gt;&lt;p&gt;Tags: 8&lt;/p&gt;</content>
  </entry>
  <entry>
    <title>command-a</title>
    <id>https://ollama.com/library/command-a</id>
    <link href="https://ollama.com/library/command-a"/>
    <summary>111 billion parameter model optimized for demanding enterprises that require fast, secure, and high-quality AI</summary>
    <updated>2025-03-13T12:39:00+00:00</updated>
    <category term="111b"/>
    <category term="tools"/>
    <content type="html">&lt;p&gt;111 billion parameter model optimized for demanding enterprises that require fast, secure, and high-quality AI&lt;/p&gt;&lt;p&gt;Pulls: 5,971&lt;/p&gt;&lt;p&gt;Tags: 5&lt;/p&gt;</content>
  </entry>
  <entry>
    <title>command-r7b-arabic</title>
    <id>https://ollama.com/library/command-r7b-arabic</id>
    <link href="https://ollama.com/library/command-r7b-arabic"/>
    <summary>A new state-of-the-art version of the lightweight Command R7B model that excels in advanced Arabic language capabilities for enterprises in the Middle East and Northern Africa.</summary>
    <updated>2025-02-28T21:28:00+00:00</updated>
    <category term="7b"/>
    <category term="tools"/>
    <content type="html">&lt;p&gt;A new state-of-the-art version of the lightweight Command R7B model that excels in advanced Arabic language capabilities for enterprises in the Middle East and Northern Africa.&lt;/p&gt;&lt;p&gt;Pulls: 4,536&lt;/p&gt;&lt;p&gt;Tags: 5&lt;/p&gt;</content>
  </entry>
  <entry>
    <title>phi4-mini</title>
    <id>https://ollama.com/library/phi4-mini</id>
    <link href="https://ollama.com/library/phi4-mini"/>
    <summary>Phi-4-mini brings significant enhancements in multilingual support, reasoning, and mathematics, and now, the long-awaited function calling feature is finally supported.</summary>
    <updated>2025-02-28T20:02:00+00:00</updated>
    <category term="3.8b"/>
    <category term="tools"/>
    <content type="html">&lt;p&gt;Phi-4-mini brings significant enhancements in multilingual support, reasoning, and mathematics, and now, the long-awaited function calling feature is finally supported.&lt;/p&gt;&lt;p&gt;Pulls: 96K&lt;/p&gt;&lt;p&gt;Tags: 5&lt;/p&gt;</content>
  </entry>
  <entry>
    <title>granite3.2-vision</title>
    <id>https://ollama.com/library/granite3.2-vision</id>
    <link href="https://ollama.com/library/granite3.2-vision"/>
    <summary>A compact and efficient vision-language model, specifically designed for visual document understanding, enabling automated content extraction from tables, charts, infographics, plots, diagrams, and more.</summary>
    <updated>2025-02-27T19:26:00+00:00</updated>
    <category term="2b"/>
    <category term="vision"/>
    <category term="tools"/>
    <content type="html">&lt;p&gt;A compact and efficient vision-language model, specifically designed for visual document understanding, enabling automated content extraction from tables, charts, infographics, plots, diagrams, and more.&lt;/p&gt;&lt;p&gt;Pulls: 35.8K&lt;/p&gt;&lt;p&gt;Tags: 5&lt;/p&gt;</content>
  </entry>
  <entry>
    <title>granite3.2</title>
    <id>https://ollama.com/library/granite3.2</id>
    <link href="https://ollama.com/library/granite3.2"/>
    <summary>Granite-3.2 is a family of long-context AI models from IBM Granite fine-tuned for thinking capabilities.</summary>
    <updated>2025-02-26T05:48:00+00:00</updated>
    <category term="2b"/>
    <category term="8b"/>
    <category term="tools"/>
    <content type="html">&lt;p&gt;Granite-3.2 is a family of long-context AI models from IBM Granite fine-tuned for thinking capabilities.&lt;/p&gt;&lt;p&gt;Pulls: 58.9K&lt;/p&gt;&lt;p&gt;Tags: 9&lt;/p&gt;</content>
  </entry>
  <entry>
    <title>r1-1776</title>
    <id>https://ollama.com/library/r1-1776</id>
    <link href="https://ollama.com/library/r1-1776"/>
    <summary>A version of the DeepSeek-R1 model that has been post trained to provide unbiased, accurate, and factual information by Perplexity.</summary>
    <updated>2025-02-21T15:28:00+00:00</updated>
    <category term="70b"/>
    <category term="671b"/>
    <content type="html">&lt;p&gt;A version of the DeepSeek-R1 model that has been post trained to provide unbiased, accurate, and factual information by Perplexity.&lt;/p&gt;&lt;p&gt;Pulls: 21.8K&lt;/p&gt;&lt;p&gt;Tags: 9&lt;/p&gt;</content>
  </entry>
  <entry>
    <title>deepscaler</title>
    <id>https://ollama.com/library/deepscaler</id>
    <link href="https://ollama.com/library/deepscaler"/>
    <summary>A fine-tuned version of Deepseek-R1-Distilled-Qwen-1.5B that surpasses the performance of OpenAI’s o1-preview with just 1.5B parameters on popular math evaluations.</summary>
    <updated>2025-02-12T01:53:00+00:00</updated>
    <category term="1.5b"/>
    <content type="html">&lt;p&gt;A fine-tuned version of Deepseek-R1-Distilled-Qwen-1.5B that surpasses the performance of OpenAI’s o1-preview with just 1.5B parameters on popular math evaluations.&lt;/p&gt;&lt;p&gt;Pulls: 72.6K&lt;/p&gt;&lt;p&gt;Tags: 5&lt;/p&gt;</content>
  </entry>
  <entry>
    <title>deepseek-r1</title>
    <id>https://ollama.com/library/deepseek-r1</id>
    <link href="https://ollama.com/library/deepseek-r1"/>
    <summary>DeepSeek's first-generation of reasoning models with comparable performance to OpenAI-o1, including six dense models distilled from DeepSeek-R1 based on Llama and Qwen.</summary>
    <updated>2025-02-07T01:40:00+00:00</updated>
    <category term="1.5b"/>
    <category term="7b"/>
    <category term="8b"/>
    <category term="14b"/>
    <category term="32b"/>
    <category term="70b"/>
    <category term="671b"/>
    <content type="html">&lt;p&gt;DeepSeek's first-generation of reasoning models with comparable performance to OpenAI-o1, including six dense models distilled from DeepSeek-R1 based on Llama and Qwen.&lt;/p&gt;&lt;p&gt;Pulls: 34.2M&lt;/p&gt;&lt;p&gt;Tags: 29&lt;/p&gt;</content>
  </entry>
  <entry>
    <title>mistral-small</title>
    <id>https://ollama.com/library/mistral-small</id>
    <link href="https://ollama.com/library/mistral-small"/>
    <summary>Mistral Small 3 sets a new benchmark in the “small” Large Language Models category below 70B.</summary>
    <updated>2025-01-30T15:58:00+00:00</updated>
    <category term="22b"/>
    <category term="24b"/>
    <category term="tools"/>
    <content type="html">&lt;p&gt;Mistral Small 3 sets a new benchmark in the “small” Large Language Models category below 70B.&lt;/p&gt;&lt;p&gt;Pulls: 474.6K&lt;/p&gt;&lt;p&gt;Tags: 21&lt;/p&gt;</content>
  </entry>
  <entry>
    <title>granite3.1-dense</title>
    <id>https://ollama.com/library/granite3.1-dense</id>
    <link href="https://ollama.com/library/granite3.1-dense"/>
    <summary>The IBM Granite 2B and 8B models are text-only dense LLMs trained on over 12 trillion tokens of data, demonstrated significant improvements over their predecessors in performance and speed in IBM’s initial testing.</summary>
    <updated>2025-01-17T22:46:00+00:00</updated>
    <category term="2b"/>
    <category term="8b"/>
    <category term="tools"/>
    <content type="html">&lt;p&gt;The IBM Granite 2B and 8B models are text-only dense LLMs trained on over 12 trillion tokens of data, demonstrated significant improvements over their predecessors in performance and speed in IBM’s initial testing.&lt;/p&gt;&lt;p&gt;Pulls: 88K&lt;/p&gt;&lt;p&gt;Tags: 33&lt;/p&gt;</content>
  </entry>
  <entry>
    <title>granite3.1-moe</title>
    <id>https://ollama.com/library/granite3.1-moe</id>
    <link href="https://ollama.com/library/granite3.1-moe"/>
    <summary>The IBM Granite 1B and 3B models are long-context mixture of experts (MoE) Granite models from IBM designed for low latency usage.</summary>
    <updated>2025-01-17T22:18:00+00:00</updated>
    <category term="1b"/>
    <category term="3b"/>
    <category term="tools"/>
    <content type="html">&lt;p&gt;The IBM Granite 1B and 3B models are long-context mixture of experts (MoE) Granite models from IBM designed for low latency usage.&lt;/p&gt;&lt;p&gt;Pulls: 39K&lt;/p&gt;&lt;p&gt;Tags: 33&lt;/p&gt;</content>
  </entry>
  <entry>
    <title>command-r7b</title>
    <id>https://ollama.com/library/command-r7b</id>
    <link href="https://ollama.com/library/command-r7b"/>
    <summary>The smallest model in Cohere's R series delivers top-tier speed, efficiency, and quality to build powerful AI applications on commodity GPUs and edge devices.</summary>
    <updated>2025-01-16T22:27:00+00:00</updated>
    <category term="7b"/>
    <category term="tools"/>
    <content type="html">&lt;p&gt;The smallest model in Cohere's R series delivers top-tier speed, efficiency, and quality to build powerful AI applications on commodity GPUs and edge devices.&lt;/p&gt;&lt;p&gt;Pulls: 29K&lt;/p&gt;&lt;p&gt;Tags: 5&lt;/p&gt;</content>
  </entry>
  <entry>
    <title>deepseek-v3</title>
    <id>https://ollama.com/library/deepseek-v3</id>
    <link href="https://ollama.com/library/deepseek-v3"/>
    <summary>A strong Mixture-of-Experts (MoE) language model with 671B total parameters with 37B activated for each token.</summary>
    <updated>2025-01-13T17:16:00+00:00</updated>
    <category term="671b"/>
    <content type="html">&lt;p&gt;A strong Mixture-of-Experts (MoE) language model with 671B total parameters with 37B activated for each token.&lt;/p&gt;&lt;p&gt;Pulls: 965.6K&lt;/p&gt;&lt;p&gt;Tags: 5&lt;/p&gt;</content>
  </entry>
  <entry>
    <title>olmo2</title>
    <id>https://ollama.com/library/olmo2</id>
    <link href="https://ollama.com/library/olmo2"/>
    <summary>OLMo 2 is a new family of 7B and 13B models trained on up to 5T tokens. These models are on par with or better than equivalently sized fully open models, and competitive with open-weight models such as Llama 3.1 on English academic benchmarks.</summary>
    <updated>2025-01-11T22:43:00+00:00</updated>
    <category term="7b"/>
    <category term="13b"/>
    <content type="html">&lt;p&gt;OLMo 2 is a new family of 7B and 13B models trained on up to 5T tokens. These models are on par with or better than equivalently sized fully open models, and competitive with open-weight models such as Llama 3.1 on English academic benchmarks.&lt;/p&gt;&lt;p&gt;Pulls: 632.6K&lt;/p&gt;&lt;p&gt;Tags: 9&lt;/p&gt;</content>
  </entry>
</feed>
