<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Ollama models</title>
  <id>https://ollama.com/search?o=newest</id>
  <author>
    <name>Model Library</name>
  </author>
  <link href="https://ollama.com/search?o=newest" rel="self"/>
  <entry>
    <title>exaone-deep</title>
    <id>https://ollama.com/library/exaone-deep</id>
    <link href="https://ollama.com/library/exaone-deep"/>
    <summary>EXAONE Deep exhibits superior capabilities in various reasoning tasks including math and coding benchmarks, ranging from 2.4B to 32B parameters developed and released by LG AI Research.</summary>
    <updated>2025-03-19T17:04:00</updated>
    <category term="2.4b"/>
    <category term="7.8b"/>
    <category term="32b"/>
    <content type="html">&lt;p&gt;EXAONE Deep exhibits superior capabilities in various reasoning tasks including math and coding benchmarks, ranging from 2.4B to 32B parameters developed and released by LG AI Research.&lt;/p&gt;&lt;p&gt;Pulls: 8,617&lt;/p&gt;&lt;p&gt;Tags: 13&lt;/p&gt;</content>
  </entry>
  <entry>
    <title>command-a</title>
    <id>https://ollama.com/library/command-a</id>
    <link href="https://ollama.com/library/command-a"/>
    <summary>111 billion parameter model optimized for demanding enterprises that require fast, secure, and high-quality AI</summary>
    <updated>2025-03-13T12:39:00</updated>
    <category term="111b"/>
    <category term="tools"/>
    <content type="html">&lt;p&gt;111 billion parameter model optimized for demanding enterprises that require fast, secure, and high-quality AI&lt;/p&gt;&lt;p&gt;Pulls: 3,352&lt;/p&gt;&lt;p&gt;Tags: 5&lt;/p&gt;</content>
  </entry>
  <entry>
    <title>gemma3</title>
    <id>https://ollama.com/library/gemma3</id>
    <link href="https://ollama.com/library/gemma3"/>
    <summary>The current, most capable model that runs on a single GPU.</summary>
    <updated>2025-03-16T01:40:00</updated>
    <category term="1b"/>
    <category term="4b"/>
    <category term="12b"/>
    <category term="27b"/>
    <category term="vision"/>
    <content type="html">&lt;p&gt;The current, most capable model that runs on a single GPU.&lt;/p&gt;&lt;p&gt;Pulls: 2.6M&lt;/p&gt;&lt;p&gt;Tags: 16&lt;/p&gt;</content>
  </entry>
  <entry>
    <title>command-r7b-arabic</title>
    <id>https://ollama.com/library/command-r7b-arabic</id>
    <link href="https://ollama.com/library/command-r7b-arabic"/>
    <summary>A new state-of-the-art version of the lightweight Command R7B model that excels in advanced Arabic language capabilities for enterprises in the Middle East and Northern Africa.</summary>
    <updated>2025-02-28T21:28:00</updated>
    <category term="7b"/>
    <category term="tools"/>
    <content type="html">&lt;p&gt;A new state-of-the-art version of the lightweight Command R7B model that excels in advanced Arabic language capabilities for enterprises in the Middle East and Northern Africa.&lt;/p&gt;&lt;p&gt;Pulls: 3,937&lt;/p&gt;&lt;p&gt;Tags: 5&lt;/p&gt;</content>
  </entry>
  <entry>
    <title>granite3.2-vision</title>
    <id>https://ollama.com/library/granite3.2-vision</id>
    <link href="https://ollama.com/library/granite3.2-vision"/>
    <summary>A compact and efficient vision-language model, specifically designed for visual document understanding, enabling automated content extraction from tables, charts, infographics, plots, diagrams, and more.</summary>
    <updated>2025-02-27T19:26:00</updated>
    <category term="2b"/>
    <category term="vision"/>
    <category term="tools"/>
    <content type="html">&lt;p&gt;A compact and efficient vision-language model, specifically designed for visual document understanding, enabling automated content extraction from tables, charts, infographics, plots, diagrams, and more.&lt;/p&gt;&lt;p&gt;Pulls: 27.3K&lt;/p&gt;&lt;p&gt;Tags: 5&lt;/p&gt;</content>
  </entry>
  <entry>
    <title>phi4-mini</title>
    <id>https://ollama.com/library/phi4-mini</id>
    <link href="https://ollama.com/library/phi4-mini"/>
    <summary>Phi-4-mini brings significant enhancements in multilingual support, reasoning, and mathematics, and now, the long-awaited function calling feature is finally supported.</summary>
    <updated>2025-02-28T20:02:00</updated>
    <category term="3.8b"/>
    <category term="tools"/>
    <content type="html">&lt;p&gt;Phi-4-mini brings significant enhancements in multilingual support, reasoning, and mathematics, and now, the long-awaited function calling feature is finally supported.&lt;/p&gt;&lt;p&gt;Pulls: 79.8K&lt;/p&gt;&lt;p&gt;Tags: 5&lt;/p&gt;</content>
  </entry>
  <entry>
    <title>granite3.2</title>
    <id>https://ollama.com/library/granite3.2</id>
    <link href="https://ollama.com/library/granite3.2"/>
    <summary>Granite-3.2 is a family of long-context AI models from IBM Granite fine-tuned for thinking capabilities.</summary>
    <updated>2025-02-26T05:48:00</updated>
    <category term="2b"/>
    <category term="8b"/>
    <category term="tools"/>
    <content type="html">&lt;p&gt;Granite-3.2 is a family of long-context AI models from IBM Granite fine-tuned for thinking capabilities.&lt;/p&gt;&lt;p&gt;Pulls: 48K&lt;/p&gt;&lt;p&gt;Tags: 9&lt;/p&gt;</content>
  </entry>
  <entry>
    <title>r1-1776</title>
    <id>https://ollama.com/library/r1-1776</id>
    <link href="https://ollama.com/library/r1-1776"/>
    <summary>A version of the DeepSeek-R1 model that has been post trained to provide unbiased, accurate, and factual information by Perplexity.</summary>
    <updated>2025-02-21T15:28:00</updated>
    <category term="70b"/>
    <category term="671b"/>
    <content type="html">&lt;p&gt;A version of the DeepSeek-R1 model that has been post trained to provide unbiased, accurate, and factual information by Perplexity.&lt;/p&gt;&lt;p&gt;Pulls: 18.7K&lt;/p&gt;&lt;p&gt;Tags: 9&lt;/p&gt;</content>
  </entry>
  <entry>
    <title>deepscaler</title>
    <id>https://ollama.com/library/deepscaler</id>
    <link href="https://ollama.com/library/deepscaler"/>
    <summary>A fine-tuned version of Deepseek-R1-Distilled-Qwen-1.5B that surpasses the performance of OpenAI‚Äôs o1-preview with just 1.5B parameters on popular math evaluations.</summary>
    <updated>2025-02-12T01:53:00</updated>
    <category term="1.5b"/>
    <content type="html">&lt;p&gt;A fine-tuned version of Deepseek-R1-Distilled-Qwen-1.5B that surpasses the performance of OpenAI‚Äôs o1-preview with just 1.5B parameters on popular math evaluations.&lt;/p&gt;&lt;p&gt;Pulls: 69.4K&lt;/p&gt;&lt;p&gt;Tags: 5&lt;/p&gt;</content>
  </entry>
  <entry>
    <title>openthinker</title>
    <id>https://ollama.com/library/openthinker</id>
    <link href="https://ollama.com/library/openthinker"/>
    <summary>A fully open-source family of reasoning models built using a dataset derived by distilling DeepSeek-R1.</summary>
    <updated>2025-02-12T19:26:00</updated>
    <category term="7b"/>
    <category term="32b"/>
    <content type="html">&lt;p&gt;A fully open-source family of reasoning models built using a dataset derived by distilling DeepSeek-R1.&lt;/p&gt;&lt;p&gt;Pulls: 513.5K&lt;/p&gt;&lt;p&gt;Tags: 9&lt;/p&gt;</content>
  </entry>
  <entry>
    <title>deepseek-r1</title>
    <id>https://ollama.com/library/deepseek-r1</id>
    <link href="https://ollama.com/library/deepseek-r1"/>
    <summary>DeepSeek's first-generation of reasoning models with comparable performance to OpenAI-o1, including six dense models distilled from DeepSeek-R1 based on Llama and Qwen.</summary>
    <updated>2025-02-07T01:40:00</updated>
    <category term="1.5b"/>
    <category term="7b"/>
    <category term="8b"/>
    <category term="14b"/>
    <category term="32b"/>
    <category term="70b"/>
    <category term="671b"/>
    <content type="html">&lt;p&gt;DeepSeek's first-generation of reasoning models with comparable performance to OpenAI-o1, including six dense models distilled from DeepSeek-R1 based on Llama and Qwen.&lt;/p&gt;&lt;p&gt;Pulls: 29.1M&lt;/p&gt;&lt;p&gt;Tags: 29&lt;/p&gt;</content>
  </entry>
  <entry>
    <title>olmo2</title>
    <id>https://ollama.com/library/olmo2</id>
    <link href="https://ollama.com/library/olmo2"/>
    <summary>OLMo 2 is a new family of 7B and 13B models trained on up to 5T tokens. These models are on par with or better than equivalently sized fully open models, and competitive with open-weight models such as Llama 3.1 on English academic benchmarks.</summary>
    <updated>2025-01-11T22:43:00</updated>
    <category term="7b"/>
    <category term="13b"/>
    <content type="html">&lt;p&gt;OLMo 2 is a new family of 7B and 13B models trained on up to 5T tokens. These models are on par with or better than equivalently sized fully open models, and competitive with open-weight models such as Llama 3.1 on English academic benchmarks.&lt;/p&gt;&lt;p&gt;Pulls: 475.2K&lt;/p&gt;&lt;p&gt;Tags: 9&lt;/p&gt;</content>
  </entry>
  <entry>
    <title>command-r7b</title>
    <id>https://ollama.com/library/command-r7b</id>
    <link href="https://ollama.com/library/command-r7b"/>
    <summary>The smallest model in Cohere's R series delivers top-tier speed, efficiency, and quality to build powerful AI applications on commodity GPUs and edge devices.</summary>
    <updated>2025-01-16T22:27:00</updated>
    <category term="7b"/>
    <category term="tools"/>
    <content type="html">&lt;p&gt;The smallest model in Cohere's R series delivers top-tier speed, efficiency, and quality to build powerful AI applications on commodity GPUs and edge devices.&lt;/p&gt;&lt;p&gt;Pulls: 26.7K&lt;/p&gt;&lt;p&gt;Tags: 5&lt;/p&gt;</content>
  </entry>
  <entry>
    <title>deepseek-v3</title>
    <id>https://ollama.com/library/deepseek-v3</id>
    <link href="https://ollama.com/library/deepseek-v3"/>
    <summary>A strong Mixture-of-Experts (MoE) language model with 671B total parameters with 37B activated for each token.</summary>
    <updated>2025-01-13T17:16:00</updated>
    <category term="671b"/>
    <content type="html">&lt;p&gt;A strong Mixture-of-Experts (MoE) language model with 671B total parameters with 37B activated for each token.&lt;/p&gt;&lt;p&gt;Pulls: 818.7K&lt;/p&gt;&lt;p&gt;Tags: 5&lt;/p&gt;</content>
  </entry>
  <entry>
    <title>phi4</title>
    <id>https://ollama.com/library/phi4</id>
    <link href="https://ollama.com/library/phi4"/>
    <summary>Phi-4 is a 14B parameter, state-of-the-art open model from Microsoft.</summary>
    <updated>2025-01-08T18:08:00</updated>
    <category term="14b"/>
    <content type="html">&lt;p&gt;Phi-4 is a 14B parameter, state-of-the-art open model from Microsoft.&lt;/p&gt;&lt;p&gt;Pulls: 1.2M&lt;/p&gt;&lt;p&gt;Tags: 5&lt;/p&gt;</content>
  </entry>
  <entry>
    <title>dolphin3</title>
    <id>https://ollama.com/library/dolphin3</id>
    <link href="https://ollama.com/library/dolphin3"/>
    <summary>Dolphin 3.0 Llama 3.1 8B üê¨ is the next generation of the Dolphin series of instruct-tuned models designed to be the ultimate general purpose local model, enabling coding, math, agentic, function calling, and general use cases.</summary>
    <updated>2025-01-05T19:38:00</updated>
    <category term="8b"/>
    <content type="html">&lt;p&gt;Dolphin 3.0 Llama 3.1 8B üê¨ is the next generation of the Dolphin series of instruct-tuned models designed to be the ultimate general purpose local model, enabling coding, math, agentic, function calling, and general use cases.&lt;/p&gt;&lt;p&gt;Pulls: 512.2K&lt;/p&gt;&lt;p&gt;Tags: 5&lt;/p&gt;</content>
  </entry>
  <entry>
    <title>smallthinker</title>
    <id>https://ollama.com/library/smallthinker</id>
    <link href="https://ollama.com/library/smallthinker"/>
    <summary>A new small reasoning model fine-tuned from the Qwen 2.5 3B Instruct model.</summary>
    <updated>2024-12-30T21:33:00</updated>
    <category term="3b"/>
    <content type="html">&lt;p&gt;A new small reasoning model fine-tuned from the Qwen 2.5 3B Instruct model.&lt;/p&gt;&lt;p&gt;Pulls: 49.6K&lt;/p&gt;&lt;p&gt;Tags: 5&lt;/p&gt;</content>
  </entry>
  <entry>
    <title>granite3.1-dense</title>
    <id>https://ollama.com/library/granite3.1-dense</id>
    <link href="https://ollama.com/library/granite3.1-dense"/>
    <summary>The IBM Granite 2B and 8B models are text-only dense LLMs trained on over 12 trillion tokens of data, demonstrated significant improvements over their predecessors in performance and speed in IBM‚Äôs initial testing.</summary>
    <updated>2025-01-17T22:46:00</updated>
    <category term="2b"/>
    <category term="8b"/>
    <category term="tools"/>
    <content type="html">&lt;p&gt;The IBM Granite 2B and 8B models are text-only dense LLMs trained on over 12 trillion tokens of data, demonstrated significant improvements over their predecessors in performance and speed in IBM‚Äôs initial testing.&lt;/p&gt;&lt;p&gt;Pulls: 84.7K&lt;/p&gt;&lt;p&gt;Tags: 33&lt;/p&gt;</content>
  </entry>
  <entry>
    <title>granite3.1-moe</title>
    <id>https://ollama.com/library/granite3.1-moe</id>
    <link href="https://ollama.com/library/granite3.1-moe"/>
    <summary>The IBM Granite 1B and 3B models are long-context mixture of experts (MoE) Granite models from IBM designed for low latency usage.</summary>
    <updated>2025-01-17T22:18:00</updated>
    <category term="1b"/>
    <category term="3b"/>
    <category term="tools"/>
    <content type="html">&lt;p&gt;The IBM Granite 1B and 3B models are long-context mixture of experts (MoE) Granite models from IBM designed for low latency usage.&lt;/p&gt;&lt;p&gt;Pulls: 37.1K&lt;/p&gt;&lt;p&gt;Tags: 33&lt;/p&gt;</content>
  </entry>
  <entry>
    <title>falcon3</title>
    <id>https://ollama.com/library/falcon3</id>
    <link href="https://ollama.com/library/falcon3"/>
    <summary>A family of efficient AI models under 10B parameters performant in science, math, and coding through innovative training techniques.</summary>
    <updated>2024-12-17T23:32:00</updated>
    <category term="1b"/>
    <category term="3b"/>
    <category term="7b"/>
    <category term="10b"/>
    <content type="html">&lt;p&gt;A family of efficient AI models under 10B parameters performant in science, math, and coding through innovative training techniques.&lt;/p&gt;&lt;p&gt;Pulls: 44.4K&lt;/p&gt;&lt;p&gt;Tags: 17&lt;/p&gt;</content>
  </entry>
  <entry>
    <title>granite-embedding</title>
    <id>https://ollama.com/library/granite-embedding</id>
    <link href="https://ollama.com/library/granite-embedding"/>
    <summary>The IBM Granite Embedding 30M and 278M models models are text-only dense biencoder embedding models, with 30M available in English only and 278M serving multilingual use cases.</summary>
    <updated>2024-12-18T04:11:00</updated>
    <category term="30m"/>
    <category term="278m"/>
    <category term="embedding"/>
    <content type="html">&lt;p&gt;The IBM Granite Embedding 30M and 278M models models are text-only dense biencoder embedding models, with 30M available in English only and 278M serving multilingual use cases.&lt;/p&gt;&lt;p&gt;Pulls: 25.3K&lt;/p&gt;&lt;p&gt;Tags: 6&lt;/p&gt;</content>
  </entry>
  <entry>
    <title>exaone3.5</title>
    <id>https://ollama.com/library/exaone3.5</id>
    <link href="https://ollama.com/library/exaone3.5"/>
    <summary>EXAONE 3.5 is a collection of instruction-tuned bilingual (English and Korean) generative models ranging from 2.4B to 32B parameters, developed and released by LG AI Research.</summary>
    <updated>2024-12-10T08:03:00</updated>
    <category term="2.4b"/>
    <category term="7.8b"/>
    <category term="32b"/>
    <content type="html">&lt;p&gt;EXAONE 3.5 is a collection of instruction-tuned bilingual (English and Korean) generative models ranging from 2.4B to 32B parameters, developed and released by LG AI Research.&lt;/p&gt;&lt;p&gt;Pulls: 31.6K&lt;/p&gt;&lt;p&gt;Tags: 13&lt;/p&gt;</content>
  </entry>
  <entry>
    <title>llama3.3</title>
    <id>https://ollama.com/library/llama3.3</id>
    <link href="https://ollama.com/library/llama3.3"/>
    <summary>New state of the art 70B model. Llama 3.3 70B offers similar performance compared to the Llama 3.1 405B model.</summary>
    <updated>2024-12-06T19:13:00</updated>
    <category term="70b"/>
    <category term="tools"/>
    <content type="html">&lt;p&gt;New state of the art 70B model. Llama 3.3 70B offers similar performance compared to the Llama 3.1 405B model.&lt;/p&gt;&lt;p&gt;Pulls: 1.6M&lt;/p&gt;&lt;p&gt;Tags: 14&lt;/p&gt;</content>
  </entry>
  <entry>
    <title>snowflake-arctic-embed2</title>
    <id>https://ollama.com/library/snowflake-arctic-embed2</id>
    <link href="https://ollama.com/library/snowflake-arctic-embed2"/>
    <summary>Snowflake's frontier embedding model. Arctic Embed 2.0 adds multilingual support without sacrificing English performance or scalability.</summary>
    <updated>2024-12-04T23:57:00</updated>
    <category term="568m"/>
    <category term="embedding"/>
    <content type="html">&lt;p&gt;Snowflake's frontier embedding model. Arctic Embed 2.0 adds multilingual support without sacrificing English performance or scalability.&lt;/p&gt;&lt;p&gt;Pulls: 47.3K&lt;/p&gt;&lt;p&gt;Tags: 3&lt;/p&gt;</content>
  </entry>
  <entry>
    <title>sailor2</title>
    <id>https://ollama.com/library/sailor2</id>
    <link href="https://ollama.com/library/sailor2"/>
    <summary>Sailor2 are multilingual language models made for South-East Asia. Available in 1B, 8B, and 20B parameter sizes.</summary>
    <updated>2024-12-04T08:10:00</updated>
    <category term="1b"/>
    <category term="8b"/>
    <category term="20b"/>
    <content type="html">&lt;p&gt;Sailor2 are multilingual language models made for South-East Asia. Available in 1B, 8B, and 20B parameter sizes.&lt;/p&gt;&lt;p&gt;Pulls: 10.1K&lt;/p&gt;&lt;p&gt;Tags: 13&lt;/p&gt;</content>
  </entry>
  <entry>
    <title>qwq</title>
    <id>https://ollama.com/library/qwq</id>
    <link href="https://ollama.com/library/qwq"/>
    <summary>QwQ is the reasoning model of the Qwen series.</summary>
    <updated>2025-03-13T18:55:00</updated>
    <category term="32b"/>
    <category term="tools"/>
    <content type="html">&lt;p&gt;QwQ is the reasoning model of the Qwen series.&lt;/p&gt;&lt;p&gt;Pulls: 1M&lt;/p&gt;&lt;p&gt;Tags: 8&lt;/p&gt;</content>
  </entry>
  <entry>
    <title>marco-o1</title>
    <id>https://ollama.com/library/marco-o1</id>
    <link href="https://ollama.com/library/marco-o1"/>
    <summary>An open large reasoning model for real-world solutions by the Alibaba International Digital Commerce Group (AIDC-AI).</summary>
    <updated>2024-12-04T08:14:00</updated>
    <category term="7b"/>
    <content type="html">&lt;p&gt;An open large reasoning model for real-world solutions by the Alibaba International Digital Commerce Group (AIDC-AI).&lt;/p&gt;&lt;p&gt;Pulls: 33.9K&lt;/p&gt;&lt;p&gt;Tags: 5&lt;/p&gt;</content>
  </entry>
  <entry>
    <title>tulu3</title>
    <id>https://ollama.com/library/tulu3</id>
    <link href="https://ollama.com/library/tulu3"/>
    <summary>T√ºlu 3 is a leading instruction following model family, offering fully open-source data, code, and recipes by the The Allen Institute for AI.</summary>
    <updated>2024-12-21T03:58:00</updated>
    <category term="8b"/>
    <category term="70b"/>
    <content type="html">&lt;p&gt;T√ºlu 3 is a leading instruction following model family, offering fully open-source data, code, and recipes by the The Allen Institute for AI.&lt;/p&gt;&lt;p&gt;Pulls: 21.3K&lt;/p&gt;&lt;p&gt;Tags: 9&lt;/p&gt;</content>
  </entry>
  <entry>
    <title>athene-v2</title>
    <id>https://ollama.com/library/athene-v2</id>
    <link href="https://ollama.com/library/athene-v2"/>
    <summary>Athene-V2 is a 72B parameter model which excels at code completion, mathematics, and log extraction tasks.</summary>
    <updated>2024-11-15T23:37:00</updated>
    <category term="72b"/>
    <category term="tools"/>
    <content type="html">&lt;p&gt;Athene-V2 is a 72B parameter model which excels at code completion, mathematics, and log extraction tasks.&lt;/p&gt;&lt;p&gt;Pulls: 77.5K&lt;/p&gt;&lt;p&gt;Tags: 17&lt;/p&gt;</content>
  </entry>
  <entry>
    <title>opencoder</title>
    <id>https://ollama.com/library/opencoder</id>
    <link href="https://ollama.com/library/opencoder"/>
    <summary>OpenCoder is an open and reproducible code LLM family which includes 1.5B and 8B models, supporting chat in English and Chinese languages.</summary>
    <updated>2024-11-18T07:31:00</updated>
    <category term="1.5b"/>
    <category term="8b"/>
    <content type="html">&lt;p&gt;OpenCoder is an open and reproducible code LLM family which includes 1.5B and 8B models, supporting chat in English and Chinese languages.&lt;/p&gt;&lt;p&gt;Pulls: 27.2K&lt;/p&gt;&lt;p&gt;Tags: 9&lt;/p&gt;</content>
  </entry>
  <entry>
    <title>llama3.2-vision</title>
    <id>https://ollama.com/library/llama3.2-vision</id>
    <link href="https://ollama.com/library/llama3.2-vision"/>
    <summary>Llama 3.2 Vision is a collection of instruction-tuned image reasoning generative models in 11B and 90B sizes.</summary>
    <updated>2024-11-06T22:17:00</updated>
    <category term="11b"/>
    <category term="90b"/>
    <category term="vision"/>
    <content type="html">&lt;p&gt;Llama 3.2 Vision is a collection of instruction-tuned image reasoning generative models in 11B and 90B sizes.&lt;/p&gt;&lt;p&gt;Pulls: 1.6M&lt;/p&gt;&lt;p&gt;Tags: 9&lt;/p&gt;</content>
  </entry>
  <entry>
    <title>smollm2</title>
    <id>https://ollama.com/library/smollm2</id>
    <link href="https://ollama.com/library/smollm2"/>
    <summary>SmolLM2 is a family of compact language models available in three size: 135M, 360M, and 1.7B parameters.</summary>
    <updated>2024-11-01T04:57:00</updated>
    <category term="135m"/>
    <category term="360m"/>
    <category term="1.7b"/>
    <category term="tools"/>
    <content type="html">&lt;p&gt;SmolLM2 is a family of compact language models available in three size: 135M, 360M, and 1.7B parameters.&lt;/p&gt;&lt;p&gt;Pulls: 436.2K&lt;/p&gt;&lt;p&gt;Tags: 49&lt;/p&gt;</content>
  </entry>
  <entry>
    <title>granite3-guardian</title>
    <id>https://ollama.com/library/granite3-guardian</id>
    <link href="https://ollama.com/library/granite3-guardian"/>
    <summary>The IBM Granite Guardian 3.0 2B and 8B models are designed to detect risks in prompts and/or responses.</summary>
    <updated>2024-11-19T23:33:00</updated>
    <category term="2b"/>
    <category term="8b"/>
    <content type="html">&lt;p&gt;The IBM Granite Guardian 3.0 2B and 8B models are designed to detect risks in prompts and/or responses.&lt;/p&gt;&lt;p&gt;Pulls: 17K&lt;/p&gt;&lt;p&gt;Tags: 10&lt;/p&gt;</content>
  </entry>
  <entry>
    <title>aya-expanse</title>
    <id>https://ollama.com/library/aya-expanse</id>
    <link href="https://ollama.com/library/aya-expanse"/>
    <summary>Cohere For AI's language models trained to perform well across 23 different languages.</summary>
    <updated>2024-10-25T02:01:00</updated>
    <category term="8b"/>
    <category term="32b"/>
    <category term="tools"/>
    <content type="html">&lt;p&gt;Cohere For AI's language models trained to perform well across 23 different languages.&lt;/p&gt;&lt;p&gt;Pulls: 48.2K&lt;/p&gt;&lt;p&gt;Tags: 33&lt;/p&gt;</content>
  </entry>
  <entry>
    <title>granite3-dense</title>
    <id>https://ollama.com/library/granite3-dense</id>
    <link href="https://ollama.com/library/granite3-dense"/>
    <summary>The IBM Granite 2B and 8B models are designed to support tool-based use cases and support for retrieval augmented generation (RAG), streamlining code generation, translation and bug fixing.</summary>
    <updated>2024-11-19T22:35:00</updated>
    <category term="2b"/>
    <category term="8b"/>
    <category term="tools"/>
    <content type="html">&lt;p&gt;The IBM Granite 2B and 8B models are designed to support tool-based use cases and support for retrieval augmented generation (RAG), streamlining code generation, translation and bug fixing.&lt;/p&gt;&lt;p&gt;Pulls: 60.9K&lt;/p&gt;&lt;p&gt;Tags: 33&lt;/p&gt;</content>
  </entry>
  <entry>
    <title>granite3-moe</title>
    <id>https://ollama.com/library/granite3-moe</id>
    <link href="https://ollama.com/library/granite3-moe"/>
    <summary>The IBM Granite 1B and 3B models are the first mixture of experts (MoE) Granite models from IBM designed for low latency usage.</summary>
    <updated>2024-11-19T22:33:00</updated>
    <category term="1b"/>
    <category term="3b"/>
    <category term="tools"/>
    <content type="html">&lt;p&gt;The IBM Granite 1B and 3B models are the first mixture of experts (MoE) Granite models from IBM designed for low latency usage.&lt;/p&gt;&lt;p&gt;Pulls: 45.4K&lt;/p&gt;&lt;p&gt;Tags: 33&lt;/p&gt;</content>
  </entry>
  <entry>
    <title>nemotron</title>
    <id>https://ollama.com/library/nemotron</id>
    <link href="https://ollama.com/library/nemotron"/>
    <summary>Llama-3.1-Nemotron-70B-Instruct is a large language model customized by NVIDIA to improve the helpfulness of LLM generated responses to user queries.</summary>
    <updated>2024-10-16T23:10:00</updated>
    <category term="70b"/>
    <category term="tools"/>
    <content type="html">&lt;p&gt;Llama-3.1-Nemotron-70B-Instruct is a large language model customized by NVIDIA to improve the helpfulness of LLM generated responses to user queries.&lt;/p&gt;&lt;p&gt;Pulls: 67.6K&lt;/p&gt;&lt;p&gt;Tags: 17&lt;/p&gt;</content>
  </entry>
  <entry>
    <title>shieldgemma</title>
    <id>https://ollama.com/library/shieldgemma</id>
    <link href="https://ollama.com/library/shieldgemma"/>
    <summary>ShieldGemma is set of instruction tuned models for evaluating the safety of text prompt input and text output responses against a set of defined safety policies.</summary>
    <updated>2024-10-11T05:58:00</updated>
    <category term="2b"/>
    <category term="9b"/>
    <category term="27b"/>
    <content type="html">&lt;p&gt;ShieldGemma is set of instruction tuned models for evaluating the safety of text prompt input and text output responses against a set of defined safety policies.&lt;/p&gt;&lt;p&gt;Pulls: 37.1K&lt;/p&gt;&lt;p&gt;Tags: 49&lt;/p&gt;</content>
  </entry>
  <entry>
    <title>llama-guard3</title>
    <id>https://ollama.com/library/llama-guard3</id>
    <link href="https://ollama.com/library/llama-guard3"/>
    <summary>Llama Guard 3 is a series of models fine-tuned for content safety classification of LLM inputs and responses.</summary>
    <updated>2024-10-11T03:32:00</updated>
    <category term="1b"/>
    <category term="8b"/>
    <content type="html">&lt;p&gt;Llama Guard 3 is a series of models fine-tuned for content safety classification of LLM inputs and responses.&lt;/p&gt;&lt;p&gt;Pulls: 32.2K&lt;/p&gt;&lt;p&gt;Tags: 33&lt;/p&gt;</content>
  </entry>
  <entry>
    <title>llama3.2</title>
    <id>https://ollama.com/library/llama3.2</id>
    <link href="https://ollama.com/library/llama3.2"/>
    <summary>Meta's Llama 3.2 goes small with 1B and 3B models.</summary>
    <updated>2024-09-25T21:09:00</updated>
    <category term="1b"/>
    <category term="3b"/>
    <category term="tools"/>
    <content type="html">&lt;p&gt;Meta's Llama 3.2 goes small with 1B and 3B models.&lt;/p&gt;&lt;p&gt;Pulls: 11.2M&lt;/p&gt;&lt;p&gt;Tags: 63&lt;/p&gt;</content>
  </entry>
  <entry>
    <title>qwen2.5-coder</title>
    <id>https://ollama.com/library/qwen2.5-coder</id>
    <link href="https://ollama.com/library/qwen2.5-coder"/>
    <summary>The latest series of Code-Specific Qwen models, with significant improvements in code generation, code reasoning, and code fixing.</summary>
    <updated>2024-11-11T23:42:00</updated>
    <category term="0.5b"/>
    <category term="1.5b"/>
    <category term="3b"/>
    <category term="7b"/>
    <category term="14b"/>
    <category term="32b"/>
    <category term="tools"/>
    <content type="html">&lt;p&gt;The latest series of Code-Specific Qwen models, with significant improvements in code generation, code reasoning, and code fixing.&lt;/p&gt;&lt;p&gt;Pulls: 4.7M&lt;/p&gt;&lt;p&gt;Tags: 196&lt;/p&gt;</content>
  </entry>
  <entry>
    <title>solar-pro</title>
    <id>https://ollama.com/library/solar-pro</id>
    <link href="https://ollama.com/library/solar-pro"/>
    <summary>Solar Pro Preview: an advanced large language model (LLM) with 22 billion parameters designed to fit into a single GPU</summary>
    <updated>2024-09-18T22:27:00</updated>
    <category term="22b"/>
    <content type="html">&lt;p&gt;Solar Pro Preview: an advanced large language model (LLM) with 22 billion parameters designed to fit into a single GPU&lt;/p&gt;&lt;p&gt;Pulls: 32.8K&lt;/p&gt;&lt;p&gt;Tags: 18&lt;/p&gt;</content>
  </entry>
  <entry>
    <title>nemotron-mini</title>
    <id>https://ollama.com/library/nemotron-mini</id>
    <link href="https://ollama.com/library/nemotron-mini"/>
    <summary>A commercial-friendly small language model by NVIDIA optimized for roleplay, RAG QA, and function calling.</summary>
    <updated>2024-09-18T20:21:00</updated>
    <category term="4b"/>
    <category term="tools"/>
    <content type="html">&lt;p&gt;A commercial-friendly small language model by NVIDIA optimized for roleplay, RAG QA, and function calling.&lt;/p&gt;&lt;p&gt;Pulls: 71.8K&lt;/p&gt;&lt;p&gt;Tags: 17&lt;/p&gt;</content>
  </entry>
  <entry>
    <title>qwen2.5</title>
    <id>https://ollama.com/library/qwen2.5</id>
    <link href="https://ollama.com/library/qwen2.5"/>
    <summary>Qwen2.5 models are pretrained on Alibaba's latest large-scale dataset, encompassing up to 18 trillion tokens. The model supports up to 128K tokens and has multilingual support.</summary>
    <updated>2024-09-19T16:42:00</updated>
    <category term="0.5b"/>
    <category term="1.5b"/>
    <category term="3b"/>
    <category term="7b"/>
    <category term="14b"/>
    <category term="32b"/>
    <category term="72b"/>
    <category term="tools"/>
    <content type="html">&lt;p&gt;Qwen2.5 models are pretrained on Alibaba's latest large-scale dataset, encompassing up to 18 trillion tokens. The model supports up to 128K tokens and has multilingual support.&lt;/p&gt;&lt;p&gt;Pulls: 5.7M&lt;/p&gt;&lt;p&gt;Tags: 133&lt;/p&gt;</content>
  </entry>
  <entry>
    <title>bespoke-minicheck</title>
    <id>https://ollama.com/library/bespoke-minicheck</id>
    <link href="https://ollama.com/library/bespoke-minicheck"/>
    <summary>A state-of-the-art fact-checking model developed by Bespoke Labs.</summary>
    <updated>2024-09-18T18:29:00</updated>
    <category term="7b"/>
    <content type="html">&lt;p&gt;A state-of-the-art fact-checking model developed by Bespoke Labs.&lt;/p&gt;&lt;p&gt;Pulls: 24.7K&lt;/p&gt;&lt;p&gt;Tags: 17&lt;/p&gt;</content>
  </entry>
  <entry>
    <title>mistral-small</title>
    <id>https://ollama.com/library/mistral-small</id>
    <link href="https://ollama.com/library/mistral-small"/>
    <summary>Mistral Small 3 sets a new benchmark in the ‚Äúsmall‚Äù Large Language Models category below 70B.</summary>
    <updated>2025-01-30T15:58:00</updated>
    <category term="22b"/>
    <category term="24b"/>
    <category term="tools"/>
    <content type="html">&lt;p&gt;Mistral Small 3 sets a new benchmark in the ‚Äúsmall‚Äù Large Language Models category below 70B.&lt;/p&gt;&lt;p&gt;Pulls: 369.2K&lt;/p&gt;&lt;p&gt;Tags: 21&lt;/p&gt;</content>
  </entry>
  <entry>
    <title>reader-lm</title>
    <id>https://ollama.com/library/reader-lm</id>
    <link href="https://ollama.com/library/reader-lm"/>
    <summary>A series of models that convert HTML content to Markdown content, which is useful for content conversion tasks.</summary>
    <updated>2024-09-11T19:13:00</updated>
    <category term="0.5b"/>
    <category term="1.5b"/>
    <content type="html">&lt;p&gt;A series of models that convert HTML content to Markdown content, which is useful for content conversion tasks.&lt;/p&gt;&lt;p&gt;Pulls: 33.7K&lt;/p&gt;&lt;p&gt;Tags: 33&lt;/p&gt;</content>
  </entry>
  <entry>
    <title>minicpm-v</title>
    <id>https://ollama.com/library/minicpm-v</id>
    <link href="https://ollama.com/library/minicpm-v"/>
    <summary>A series of multimodal LLMs (MLLMs) designed for vision-language understanding.</summary>
    <updated>2024-11-18T07:03:00</updated>
    <category term="8b"/>
    <category term="vision"/>
    <content type="html">&lt;p&gt;A series of multimodal LLMs (MLLMs) designed for vision-language understanding.&lt;/p&gt;&lt;p&gt;Pulls: 667.8K&lt;/p&gt;&lt;p&gt;Tags: 17&lt;/p&gt;</content>
  </entry>
  <entry>
    <title>deepseek-v2.5</title>
    <id>https://ollama.com/library/deepseek-v2.5</id>
    <link href="https://ollama.com/library/deepseek-v2.5"/>
    <summary>An upgraded version of DeekSeek-V2  that integrates the general and coding abilities of both DeepSeek-V2-Chat and DeepSeek-Coder-V2-Instruct.</summary>
    <updated>2024-09-11T19:31:00</updated>
    <category term="236b"/>
    <content type="html">&lt;p&gt;An upgraded version of DeekSeek-V2  that integrates the general and coding abilities of both DeepSeek-V2-Chat and DeepSeek-Coder-V2-Instruct.&lt;/p&gt;&lt;p&gt;Pulls: 50.4K&lt;/p&gt;&lt;p&gt;Tags: 7&lt;/p&gt;</content>
  </entry>
  <entry>
    <title>reflection</title>
    <id>https://ollama.com/library/reflection</id>
    <link href="https://ollama.com/library/reflection"/>
    <summary>A high-performing model trained with a new technique called Reflection-tuning that teaches a LLM to detect mistakes in its reasoning and correct course.</summary>
    <updated>2024-09-09T05:16:00</updated>
    <category term="70b"/>
    <content type="html">&lt;p&gt;A high-performing model trained with a new technique called Reflection-tuning that teaches a LLM to detect mistakes in its reasoning and correct course.&lt;/p&gt;&lt;p&gt;Pulls: 103.8K&lt;/p&gt;&lt;p&gt;Tags: 17&lt;/p&gt;</content>
  </entry>
  <entry>
    <title>yi-coder</title>
    <id>https://ollama.com/library/yi-coder</id>
    <link href="https://ollama.com/library/yi-coder"/>
    <summary>Yi-Coder is a series of open-source code language models that delivers state-of-the-art coding performance with fewer than 10 billion parameters.</summary>
    <updated>2024-09-11T16:47:00</updated>
    <category term="1.5b"/>
    <category term="9b"/>
    <content type="html">&lt;p&gt;Yi-Coder is a series of open-source code language models that delivers state-of-the-art coding performance with fewer than 10 billion parameters.&lt;/p&gt;&lt;p&gt;Pulls: 78.1K&lt;/p&gt;&lt;p&gt;Tags: 67&lt;/p&gt;</content>
  </entry>
  <entry>
    <title>qwen2-math</title>
    <id>https://ollama.com/library/qwen2-math</id>
    <link href="https://ollama.com/library/qwen2-math"/>
    <summary>Qwen2 Math is a series of specialized math language models built upon the Qwen2 LLMs, which significantly outperforms the mathematical capabilities of open-source models and even closed-source models (e.g., GPT4o).</summary>
    <updated>2024-08-30T21:09:00</updated>
    <category term="1.5b"/>
    <category term="7b"/>
    <category term="72b"/>
    <content type="html">&lt;p&gt;Qwen2 Math is a series of specialized math language models built upon the Qwen2 LLMs, which significantly outperforms the mathematical capabilities of open-source models and even closed-source models (e.g., GPT4o).&lt;/p&gt;&lt;p&gt;Pulls: 121.9K&lt;/p&gt;&lt;p&gt;Tags: 52&lt;/p&gt;</content>
  </entry>
  <entry>
    <title>hermes3</title>
    <id>https://ollama.com/library/hermes3</id>
    <link href="https://ollama.com/library/hermes3"/>
    <summary>Hermes 3 is the latest version of the flagship Hermes series of LLMs by Nous Research</summary>
    <updated>2024-12-16T08:56:00</updated>
    <category term="3b"/>
    <category term="8b"/>
    <category term="70b"/>
    <category term="405b"/>
    <category term="tools"/>
    <content type="html">&lt;p&gt;Hermes 3 is the latest version of the flagship Hermes series of LLMs by Nous Research&lt;/p&gt;&lt;p&gt;Pulls: 267.6K&lt;/p&gt;&lt;p&gt;Tags: 65&lt;/p&gt;</content>
  </entry>
  <entry>
    <title>phi3.5</title>
    <id>https://ollama.com/library/phi3.5</id>
    <link href="https://ollama.com/library/phi3.5"/>
    <summary>A lightweight AI model with 3.8 billion parameters with performance overtaking similarly and larger sized models.</summary>
    <updated>2024-09-05T19:00:00</updated>
    <category term="3.8b"/>
    <content type="html">&lt;p&gt;A lightweight AI model with 3.8 billion parameters with performance overtaking similarly and larger sized models.&lt;/p&gt;&lt;p&gt;Pulls: 254.3K&lt;/p&gt;&lt;p&gt;Tags: 17&lt;/p&gt;</content>
  </entry>
  <entry>
    <title>smollm</title>
    <id>https://ollama.com/library/smollm</id>
    <link href="https://ollama.com/library/smollm"/>
    <summary>ü™ê A family of small models with 135M, 360M, and 1.7B parameters, trained on a new high-quality dataset.</summary>
    <updated>2024-08-20T17:04:00</updated>
    <category term="135m"/>
    <category term="360m"/>
    <category term="1.7b"/>
    <content type="html">&lt;p&gt;ü™ê A family of small models with 135M, 360M, and 1.7B parameters, trained on a new high-quality dataset.&lt;/p&gt;&lt;p&gt;Pulls: 197.3K&lt;/p&gt;&lt;p&gt;Tags: 94&lt;/p&gt;</content>
  </entry>
  <entry>
    <title>bge-large</title>
    <id>https://ollama.com/library/bge-large</id>
    <link href="https://ollama.com/library/bge-large"/>
    <summary>Embedding model from BAAI mapping texts to vectors.</summary>
    <updated>2024-08-07T00:04:00</updated>
    <category term="335m"/>
    <category term="embedding"/>
    <content type="html">&lt;p&gt;Embedding model from BAAI mapping texts to vectors.&lt;/p&gt;&lt;p&gt;Pulls: 92.5K&lt;/p&gt;&lt;p&gt;Tags: 3&lt;/p&gt;</content>
  </entry>
  <entry>
    <title>paraphrase-multilingual</title>
    <id>https://ollama.com/library/paraphrase-multilingual</id>
    <link href="https://ollama.com/library/paraphrase-multilingual"/>
    <summary>Sentence-transformers model that can be used for tasks like clustering or semantic search.</summary>
    <updated>2024-08-07T00:05:00</updated>
    <category term="278m"/>
    <category term="embedding"/>
    <content type="html">&lt;p&gt;Sentence-transformers model that can be used for tasks like clustering or semantic search.&lt;/p&gt;&lt;p&gt;Pulls: 54.1K&lt;/p&gt;&lt;p&gt;Tags: 3&lt;/p&gt;</content>
  </entry>
  <entry>
    <title>bge-m3</title>
    <id>https://ollama.com/library/bge-m3</id>
    <link href="https://ollama.com/library/bge-m3"/>
    <summary>BGE-M3 is a new model from BAAI distinguished for its versatility in Multi-Functionality, Multi-Linguality, and Multi-Granularity.</summary>
    <updated>2024-08-07T00:07:00</updated>
    <category term="567m"/>
    <category term="embedding"/>
    <content type="html">&lt;p&gt;BGE-M3 is a new model from BAAI distinguished for its versatility in Multi-Functionality, Multi-Linguality, and Multi-Granularity.&lt;/p&gt;&lt;p&gt;Pulls: 678.8K&lt;/p&gt;&lt;p&gt;Tags: 3&lt;/p&gt;</content>
  </entry>
  <entry>
    <title>mistral-large</title>
    <id>https://ollama.com/library/mistral-large</id>
    <link href="https://ollama.com/library/mistral-large"/>
    <summary>Mistral Large 2 is Mistral's new flagship model that is significantly more capable in code generation, mathematics, and reasoning with 128k context window and support for dozens of languages.</summary>
    <updated>2024-11-22T21:28:00</updated>
    <category term="123b"/>
    <category term="tools"/>
    <content type="html">&lt;p&gt;Mistral Large 2 is Mistral's new flagship model that is significantly more capable in code generation, mathematics, and reasoning with 128k context window and support for dozens of languages.&lt;/p&gt;&lt;p&gt;Pulls: 126.7K&lt;/p&gt;&lt;p&gt;Tags: 32&lt;/p&gt;</content>
  </entry>
  <entry>
    <title>llama3.1</title>
    <id>https://ollama.com/library/llama3.1</id>
    <link href="https://ollama.com/library/llama3.1"/>
    <summary>Llama 3.1 is a new state-of-the-art model from Meta available in 8B, 70B and 405B parameter sizes.</summary>
    <updated>2024-11-30T22:34:00</updated>
    <category term="8b"/>
    <category term="70b"/>
    <category term="405b"/>
    <category term="tools"/>
    <content type="html">&lt;p&gt;Llama 3.1 is a new state-of-the-art model from Meta available in 8B, 70B and 405B parameter sizes.&lt;/p&gt;&lt;p&gt;Pulls: 46M&lt;/p&gt;&lt;p&gt;Tags: 93&lt;/p&gt;</content>
  </entry>
  <entry>
    <title>nuextract</title>
    <id>https://ollama.com/library/nuextract</id>
    <link href="https://ollama.com/library/nuextract"/>
    <summary>A 3.8B model fine-tuned on a private high-quality synthetic dataset for information extraction, based on Phi-3.</summary>
    <updated>2024-07-22T23:54:00</updated>
    <category term="3.8b"/>
    <content type="html">&lt;p&gt;A 3.8B model fine-tuned on a private high-quality synthetic dataset for information extraction, based on Phi-3.&lt;/p&gt;&lt;p&gt;Pulls: 27.3K&lt;/p&gt;&lt;p&gt;Tags: 17&lt;/p&gt;</content>
  </entry>
  <entry>
    <title>mistral-nemo</title>
    <id>https://ollama.com/library/mistral-nemo</id>
    <link href="https://ollama.com/library/mistral-nemo"/>
    <summary>A state-of-the-art 12B model with 128k context length, built by Mistral AI in collaboration with NVIDIA.</summary>
    <updated>2024-08-05T22:14:00</updated>
    <category term="12b"/>
    <category term="tools"/>
    <content type="html">&lt;p&gt;A state-of-the-art 12B model with 128k context length, built by Mistral AI in collaboration with NVIDIA.&lt;/p&gt;&lt;p&gt;Pulls: 1.4M&lt;/p&gt;&lt;p&gt;Tags: 17&lt;/p&gt;</content>
  </entry>
  <entry>
    <title>firefunction-v2</title>
    <id>https://ollama.com/library/firefunction-v2</id>
    <link href="https://ollama.com/library/firefunction-v2"/>
    <summary>An open weights function calling model based on Llama 3, competitive with GPT-4o function calling capabilities.</summary>
    <updated>2024-07-18T16:39:00</updated>
    <category term="70b"/>
    <category term="tools"/>
    <content type="html">&lt;p&gt;An open weights function calling model based on Llama 3, competitive with GPT-4o function calling capabilities.&lt;/p&gt;&lt;p&gt;Pulls: 19.4K&lt;/p&gt;&lt;p&gt;Tags: 17&lt;/p&gt;</content>
  </entry>
  <entry>
    <title>llama3-groq-tool-use</title>
    <id>https://ollama.com/library/llama3-groq-tool-use</id>
    <link href="https://ollama.com/library/llama3-groq-tool-use"/>
    <summary>A series of models from Groq that represent a significant advancement in open-source AI capabilities for tool use/function calling.</summary>
    <updated>2024-07-25T01:15:00</updated>
    <category term="8b"/>
    <category term="70b"/>
    <category term="tools"/>
    <content type="html">&lt;p&gt;A series of models from Groq that represent a significant advancement in open-source AI capabilities for tool use/function calling.&lt;/p&gt;&lt;p&gt;Pulls: 57.9K&lt;/p&gt;&lt;p&gt;Tags: 33&lt;/p&gt;</content>
  </entry>
  <entry>
    <title>mathstral</title>
    <id>https://ollama.com/library/mathstral</id>
    <link href="https://ollama.com/library/mathstral"/>
    <summary>MathŒ£tral: a 7B model designed for math reasoning and scientific discovery by Mistral AI.</summary>
    <updated>2024-07-16T17:04:00</updated>
    <category term="7b"/>
    <content type="html">&lt;p&gt;MathŒ£tral: a 7B model designed for math reasoning and scientific discovery by Mistral AI.&lt;/p&gt;&lt;p&gt;Pulls: 33.4K&lt;/p&gt;&lt;p&gt;Tags: 17&lt;/p&gt;</content>
  </entry>
  <entry>
    <title>codegeex4</title>
    <id>https://ollama.com/library/codegeex4</id>
    <link href="https://ollama.com/library/codegeex4"/>
    <summary>A versatile model for AI software development scenarios, including code completion.</summary>
    <updated>2024-07-09T01:33:00</updated>
    <category term="9b"/>
    <content type="html">&lt;p&gt;A versatile model for AI software development scenarios, including code completion.&lt;/p&gt;&lt;p&gt;Pulls: 140.3K&lt;/p&gt;&lt;p&gt;Tags: 17&lt;/p&gt;</content>
  </entry>
  <entry>
    <title>glm4</title>
    <id>https://ollama.com/library/glm4</id>
    <link href="https://ollama.com/library/glm4"/>
    <summary>A strong multi-lingual general language model with competitive performance to Llama 3.</summary>
    <updated>2024-07-09T01:21:00</updated>
    <category term="9b"/>
    <content type="html">&lt;p&gt;A strong multi-lingual general language model with competitive performance to Llama 3.&lt;/p&gt;&lt;p&gt;Pulls: 125.2K&lt;/p&gt;&lt;p&gt;Tags: 32&lt;/p&gt;</content>
  </entry>
  <entry>
    <title>internlm2</title>
    <id>https://ollama.com/library/internlm2</id>
    <link href="https://ollama.com/library/internlm2"/>
    <summary>InternLM2.5 is a 7B parameter model tailored for practical scenarios with outstanding reasoning capability.</summary>
    <updated>2024-08-16T21:19:00</updated>
    <category term="1m"/>
    <category term="1.8b"/>
    <category term="7b"/>
    <category term="20b"/>
    <content type="html">&lt;p&gt;InternLM2.5 is a 7B parameter model tailored for practical scenarios with outstanding reasoning capability.&lt;/p&gt;&lt;p&gt;Pulls: 75K&lt;/p&gt;&lt;p&gt;Tags: 65&lt;/p&gt;</content>
  </entry>
  <entry>
    <title>gemma2</title>
    <id>https://ollama.com/library/gemma2</id>
    <link href="https://ollama.com/library/gemma2"/>
    <summary>Google Gemma 2 is a high-performing and efficient model available in three sizes: 2B, 9B, and 27B.</summary>
    <updated>2024-08-01T18:51:00</updated>
    <category term="2b"/>
    <category term="9b"/>
    <category term="27b"/>
    <content type="html">&lt;p&gt;Google Gemma 2 is a high-performing and efficient model available in three sizes: 2B, 9B, and 27B.&lt;/p&gt;&lt;p&gt;Pulls: 3.6M&lt;/p&gt;&lt;p&gt;Tags: 94&lt;/p&gt;</content>
  </entry>
  <entry>
    <title>deepseek-coder-v2</title>
    <id>https://ollama.com/library/deepseek-coder-v2</id>
    <link href="https://ollama.com/library/deepseek-coder-v2"/>
    <summary>An open-source Mixture-of-Experts code language model that achieves performance comparable to GPT4-Turbo in code-specific tasks.</summary>
    <updated>2024-09-06T03:33:00</updated>
    <category term="16b"/>
    <category term="236b"/>
    <content type="html">&lt;p&gt;An open-source Mixture-of-Experts code language model that achieves performance comparable to GPT4-Turbo in code-specific tasks.&lt;/p&gt;&lt;p&gt;Pulls: 741.6K&lt;/p&gt;&lt;p&gt;Tags: 64&lt;/p&gt;</content>
  </entry>
  <entry>
    <title>qwen2</title>
    <id>https://ollama.com/library/qwen2</id>
    <link href="https://ollama.com/library/qwen2"/>
    <summary>Qwen2 is a new series of large language models from Alibaba group</summary>
    <updated>2024-09-11T02:17:00</updated>
    <category term="0.5b"/>
    <category term="1.5b"/>
    <category term="7b"/>
    <category term="72b"/>
    <category term="tools"/>
    <content type="html">&lt;p&gt;Qwen2 is a new series of large language models from Alibaba group&lt;/p&gt;&lt;p&gt;Pulls: 4.2M&lt;/p&gt;&lt;p&gt;Tags: 97&lt;/p&gt;</content>
  </entry>
  <entry>
    <title>deepseek-v2</title>
    <id>https://ollama.com/library/deepseek-v2</id>
    <link href="https://ollama.com/library/deepseek-v2"/>
    <summary>A strong, economical, and efficient Mixture-of-Experts language model.</summary>
    <updated>2024-06-22T12:56:00</updated>
    <category term="16b"/>
    <category term="236b"/>
    <content type="html">&lt;p&gt;A strong, economical, and efficient Mixture-of-Experts language model.&lt;/p&gt;&lt;p&gt;Pulls: 131.8K&lt;/p&gt;&lt;p&gt;Tags: 34&lt;/p&gt;</content>
  </entry>
  <entry>
    <title>codestral</title>
    <id>https://ollama.com/library/codestral</id>
    <link href="https://ollama.com/library/codestral"/>
    <summary>Codestral is Mistral AI‚Äôs first-ever code model designed for code generation tasks.</summary>
    <updated>2024-09-03T21:32:00</updated>
    <category term="22b"/>
    <content type="html">&lt;p&gt;Codestral is Mistral AI‚Äôs first-ever code model designed for code generation tasks.&lt;/p&gt;&lt;p&gt;Pulls: 233.7K&lt;/p&gt;&lt;p&gt;Tags: 17&lt;/p&gt;</content>
  </entry>
  <entry>
    <title>granite-code</title>
    <id>https://ollama.com/library/granite-code</id>
    <link href="https://ollama.com/library/granite-code"/>
    <summary>A family of open foundation models by IBM for Code Intelligence</summary>
    <updated>2024-09-03T00:45:00</updated>
    <category term="3b"/>
    <category term="8b"/>
    <category term="20b"/>
    <category term="34b"/>
    <content type="html">&lt;p&gt;A family of open foundation models by IBM for Code Intelligence&lt;/p&gt;&lt;p&gt;Pulls: 194.4K&lt;/p&gt;&lt;p&gt;Tags: 162&lt;/p&gt;</content>
  </entry>
  <entry>
    <title>aya</title>
    <id>https://ollama.com/library/aya</id>
    <link href="https://ollama.com/library/aya"/>
    <summary>Aya 23, released by Cohere, is a new family of state-of-the-art, multilingual models that support 23 languages.</summary>
    <updated>2024-05-23T20:29:00</updated>
    <category term="8b"/>
    <category term="35b"/>
    <content type="html">&lt;p&gt;Aya 23, released by Cohere, is a new family of state-of-the-art, multilingual models that support 23 languages.&lt;/p&gt;&lt;p&gt;Pulls: 137.7K&lt;/p&gt;&lt;p&gt;Tags: 33&lt;/p&gt;</content>
  </entry>
  <entry>
    <title>falcon2</title>
    <id>https://ollama.com/library/falcon2</id>
    <link href="https://ollama.com/library/falcon2"/>
    <summary>Falcon2 is an 11B parameters causal decoder-only model built by TII and trained over 5T tokens.</summary>
    <updated>2024-05-14T01:46:00</updated>
    <category term="11b"/>
    <content type="html">&lt;p&gt;Falcon2 is an 11B parameters causal decoder-only model built by TII and trained over 5T tokens.&lt;/p&gt;&lt;p&gt;Pulls: 32.8K&lt;/p&gt;&lt;p&gt;Tags: 17&lt;/p&gt;</content>
  </entry>
  <entry>
    <title>llama3-chatqa</title>
    <id>https://ollama.com/library/llama3-chatqa</id>
    <link href="https://ollama.com/library/llama3-chatqa"/>
    <summary>A model from NVIDIA based on Llama 3 that excels at conversational question answering (QA) and retrieval-augmented generation (RAG).</summary>
    <updated>2024-05-10T23:53:00</updated>
    <category term="8b"/>
    <category term="70b"/>
    <content type="html">&lt;p&gt;A model from NVIDIA based on Llama 3 that excels at conversational question answering (QA) and retrieval-augmented generation (RAG).&lt;/p&gt;&lt;p&gt;Pulls: 98.7K&lt;/p&gt;&lt;p&gt;Tags: 35&lt;/p&gt;</content>
  </entry>
  <entry>
    <title>llava-phi3</title>
    <id>https://ollama.com/library/llava-phi3</id>
    <link href="https://ollama.com/library/llava-phi3"/>
    <summary>A new small LLaVA model fine-tuned from Phi 3 Mini.</summary>
    <updated>2024-05-07T21:20:00</updated>
    <category term="3.8b"/>
    <category term="vision"/>
    <content type="html">&lt;p&gt;A new small LLaVA model fine-tuned from Phi 3 Mini.&lt;/p&gt;&lt;p&gt;Pulls: 83.3K&lt;/p&gt;&lt;p&gt;Tags: 4&lt;/p&gt;</content>
  </entry>
  <entry>
    <title>llava-llama3</title>
    <id>https://ollama.com/library/llava-llama3</id>
    <link href="https://ollama.com/library/llava-llama3"/>
    <summary>A LLaVA model fine-tuned from Llama 3 Instruct with better scores in several benchmarks.</summary>
    <updated>2024-05-07T21:01:00</updated>
    <category term="8b"/>
    <category term="vision"/>
    <content type="html">&lt;p&gt;A LLaVA model fine-tuned from Llama 3 Instruct with better scores in several benchmarks.&lt;/p&gt;&lt;p&gt;Pulls: 514.1K&lt;/p&gt;&lt;p&gt;Tags: 4&lt;/p&gt;</content>
  </entry>
  <entry>
    <title>llama3-gradient</title>
    <id>https://ollama.com/library/llama3-gradient</id>
    <link href="https://ollama.com/library/llama3-gradient"/>
    <summary>This model extends LLama-3 8B's context length from 8k to over 1m tokens.</summary>
    <updated>2024-05-05T01:05:00</updated>
    <category term="8b"/>
    <category term="70b"/>
    <content type="html">&lt;p&gt;This model extends LLama-3 8B's context length from 8k to over 1m tokens.&lt;/p&gt;&lt;p&gt;Pulls: 98.8K&lt;/p&gt;&lt;p&gt;Tags: 35&lt;/p&gt;</content>
  </entry>
  <entry>
    <title>moondream</title>
    <id>https://ollama.com/library/moondream</id>
    <link href="https://ollama.com/library/moondream"/>
    <summary>moondream2 is a small vision language model designed to run efficiently on edge devices.</summary>
    <updated>2024-05-07T21:18:00</updated>
    <category term="1.8b"/>
    <category term="vision"/>
    <content type="html">&lt;p&gt;moondream2 is a small vision language model designed to run efficiently on edge devices.&lt;/p&gt;&lt;p&gt;Pulls: 147.7K&lt;/p&gt;&lt;p&gt;Tags: 18&lt;/p&gt;</content>
  </entry>
  <entry>
    <title>phi3</title>
    <id>https://ollama.com/library/phi3</id>
    <link href="https://ollama.com/library/phi3"/>
    <summary>Phi-3 is a family of lightweight 3B (Mini) and 14B (Medium) state-of-the-art open models by Microsoft.</summary>
    <updated>2024-07-30T18:53:00</updated>
    <category term="3.8b"/>
    <category term="14b"/>
    <content type="html">&lt;p&gt;Phi-3 is a family of lightweight 3B (Mini) and 14B (Medium) state-of-the-art open models by Microsoft.&lt;/p&gt;&lt;p&gt;Pulls: 3M&lt;/p&gt;&lt;p&gt;Tags: 72&lt;/p&gt;</content>
  </entry>
  <entry>
    <title>dolphin-llama3</title>
    <id>https://ollama.com/library/dolphin-llama3</id>
    <link href="https://ollama.com/library/dolphin-llama3"/>
    <summary>Dolphin 2.9 is a new model with 8B and 70B sizes by Eric Hartford based on Llama 3 that has a variety of instruction, conversational, and coding skills.</summary>
    <updated>2024-05-10T22:04:00</updated>
    <category term="8b"/>
    <category term="70b"/>
    <content type="html">&lt;p&gt;Dolphin 2.9 is a new model with 8B and 70B sizes by Eric Hartford based on Llama 3 that has a variety of instruction, conversational, and coding skills.&lt;/p&gt;&lt;p&gt;Pulls: 299.1K&lt;/p&gt;&lt;p&gt;Tags: 53&lt;/p&gt;</content>
  </entry>
  <entry>
    <title>llama3</title>
    <id>https://ollama.com/library/llama3</id>
    <link href="https://ollama.com/library/llama3"/>
    <summary>Meta Llama 3: The most capable openly available LLM to date</summary>
    <updated>2024-05-21T04:54:00</updated>
    <category term="8b"/>
    <category term="70b"/>
    <content type="html">&lt;p&gt;Meta Llama 3: The most capable openly available LLM to date&lt;/p&gt;&lt;p&gt;Pulls: 7.7M&lt;/p&gt;&lt;p&gt;Tags: 68&lt;/p&gt;</content>
  </entry>
  <entry>
    <title>codeqwen</title>
    <id>https://ollama.com/library/codeqwen</id>
    <link href="https://ollama.com/library/codeqwen"/>
    <summary>CodeQwen1.5 is a large language model pretrained on a large amount of code data.</summary>
    <updated>2024-06-24T07:00:00</updated>
    <category term="7b"/>
    <content type="html">&lt;p&gt;CodeQwen1.5 is a large language model pretrained on a large amount of code data.&lt;/p&gt;&lt;p&gt;Pulls: 136.4K&lt;/p&gt;&lt;p&gt;Tags: 30&lt;/p&gt;</content>
  </entry>
  <entry>
    <title>snowflake-arctic-embed</title>
    <id>https://ollama.com/library/snowflake-arctic-embed</id>
    <link href="https://ollama.com/library/snowflake-arctic-embed"/>
    <summary>A suite of text embedding models by Snowflake, optimized for performance.</summary>
    <updated>2024-04-16T16:06:00</updated>
    <category term="22m"/>
    <category term="33m"/>
    <category term="110m"/>
    <category term="137m"/>
    <category term="335m"/>
    <category term="embedding"/>
    <content type="html">&lt;p&gt;A suite of text embedding models by Snowflake, optimized for performance.&lt;/p&gt;&lt;p&gt;Pulls: 700.8K&lt;/p&gt;&lt;p&gt;Tags: 16&lt;/p&gt;</content>
  </entry>
  <entry>
    <title>dbrx</title>
    <id>https://ollama.com/library/dbrx</id>
    <link href="https://ollama.com/library/dbrx"/>
    <summary>DBRX is an open, general-purpose LLM created by Databricks.</summary>
    <updated>2024-04-16T04:18:00</updated>
    <category term="132b"/>
    <content type="html">&lt;p&gt;DBRX is an open, general-purpose LLM created by Databricks.&lt;/p&gt;&lt;p&gt;Pulls: 18.5K&lt;/p&gt;&lt;p&gt;Tags: 7&lt;/p&gt;</content>
  </entry>
  <entry>
    <title>command-r-plus</title>
    <id>https://ollama.com/library/command-r-plus</id>
    <link href="https://ollama.com/library/command-r-plus"/>
    <summary>Command R+ is a powerful, scalable large language model purpose-built to excel at real-world enterprise use cases.</summary>
    <updated>2024-08-31T07:16:00</updated>
    <category term="104b"/>
    <category term="tools"/>
    <content type="html">&lt;p&gt;Command R+ is a powerful, scalable large language model purpose-built to excel at real-world enterprise use cases.&lt;/p&gt;&lt;p&gt;Pulls: 120.4K&lt;/p&gt;&lt;p&gt;Tags: 21&lt;/p&gt;</content>
  </entry>
  <entry>
    <title>wizardlm2</title>
    <id>https://ollama.com/library/wizardlm2</id>
    <link href="https://ollama.com/library/wizardlm2"/>
    <summary>State of the art large language model from Microsoft AI with improved performance on complex chat, multilingual, reasoning and agent use cases.</summary>
    <updated>2024-04-16T00:46:00</updated>
    <category term="7b"/>
    <category term="8x22b"/>
    <content type="html">&lt;p&gt;State of the art large language model from Microsoft AI with improved performance on complex chat, multilingual, reasoning and agent use cases.&lt;/p&gt;&lt;p&gt;Pulls: 358.5K&lt;/p&gt;&lt;p&gt;Tags: 22&lt;/p&gt;</content>
  </entry>
  <entry>
    <title>codegemma</title>
    <id>https://ollama.com/library/codegemma</id>
    <link href="https://ollama.com/library/codegemma"/>
    <summary>CodeGemma is a collection of powerful, lightweight models that can perform a variety of coding tasks like fill-in-the-middle code completion, code generation, natural language understanding, mathematical reasoning, and instruction following.</summary>
    <updated>2024-07-18T17:38:00</updated>
    <category term="2b"/>
    <category term="7b"/>
    <content type="html">&lt;p&gt;CodeGemma is a collection of powerful, lightweight models that can perform a variety of coding tasks like fill-in-the-middle code completion, code generation, natural language understanding, mathematical reasoning, and instruction following.&lt;/p&gt;&lt;p&gt;Pulls: 546.6K&lt;/p&gt;&lt;p&gt;Tags: 85&lt;/p&gt;</content>
  </entry>
  <entry>
    <title>command-r</title>
    <id>https://ollama.com/library/command-r</id>
    <link href="https://ollama.com/library/command-r"/>
    <summary>Command R is a Large Language Model optimized for conversational interaction and long context tasks.</summary>
    <updated>2024-08-30T23:38:00</updated>
    <category term="35b"/>
    <category term="tools"/>
    <content type="html">&lt;p&gt;Command R is a Large Language Model optimized for conversational interaction and long context tasks.&lt;/p&gt;&lt;p&gt;Pulls: 285.4K&lt;/p&gt;&lt;p&gt;Tags: 32&lt;/p&gt;</content>
  </entry>
  <entry>
    <title>mxbai-embed-large</title>
    <id>https://ollama.com/library/mxbai-embed-large</id>
    <link href="https://ollama.com/library/mxbai-embed-large"/>
    <summary>State-of-the-art large embedding model from mixedbread.ai</summary>
    <updated>2024-05-06T23:36:00</updated>
    <category term="335m"/>
    <category term="embedding"/>
    <content type="html">&lt;p&gt;State-of-the-art large embedding model from mixedbread.ai&lt;/p&gt;&lt;p&gt;Pulls: 1.9M&lt;/p&gt;&lt;p&gt;Tags: 4&lt;/p&gt;</content>
  </entry>
  <entry>
    <title>dolphincoder</title>
    <id>https://ollama.com/library/dolphincoder</id>
    <link href="https://ollama.com/library/dolphincoder"/>
    <summary>A 7B and 15B uncensored variant of the Dolphin model family that excels at coding, based on StarCoder2.</summary>
    <updated>2024-04-09T00:01:00</updated>
    <category term="7b"/>
    <category term="15b"/>
    <content type="html">&lt;p&gt;A 7B and 15B uncensored variant of the Dolphin model family that excels at coding, based on StarCoder2.&lt;/p&gt;&lt;p&gt;Pulls: 85.1K&lt;/p&gt;&lt;p&gt;Tags: 35&lt;/p&gt;</content>
  </entry>
  <entry>
    <title>starcoder2</title>
    <id>https://ollama.com/library/starcoder2</id>
    <link href="https://ollama.com/library/starcoder2"/>
    <summary>StarCoder2 is the next generation of transparently trained open code LLMs that comes in three sizes: 3B, 7B and 15B parameters.</summary>
    <updated>2024-09-06T23:42:00</updated>
    <category term="3b"/>
    <category term="7b"/>
    <category term="15b"/>
    <content type="html">&lt;p&gt;StarCoder2 is the next generation of transparently trained open code LLMs that comes in three sizes: 3B, 7B and 15B parameters.&lt;/p&gt;&lt;p&gt;Pulls: 903.7K&lt;/p&gt;&lt;p&gt;Tags: 67&lt;/p&gt;</content>
  </entry>
  <entry>
    <title>all-minilm</title>
    <id>https://ollama.com/library/all-minilm</id>
    <link href="https://ollama.com/library/all-minilm"/>
    <summary>Embedding models on very large sentence level datasets.</summary>
    <updated>2024-05-06T23:38:00</updated>
    <category term="22m"/>
    <category term="33m"/>
    <category term="embedding"/>
    <content type="html">&lt;p&gt;Embedding models on very large sentence level datasets.&lt;/p&gt;&lt;p&gt;Pulls: 328.7K&lt;/p&gt;&lt;p&gt;Tags: 10&lt;/p&gt;</content>
  </entry>
  <entry>
    <title>nomic-embed-text</title>
    <id>https://ollama.com/library/nomic-embed-text</id>
    <link href="https://ollama.com/library/nomic-embed-text"/>
    <summary>A high-performing open embedding model with a large token context window.</summary>
    <updated>2024-02-21T17:26:00</updated>
    <category term="embedding"/>
    <content type="html">&lt;p&gt;A high-performing open embedding model with a large token context window.&lt;/p&gt;&lt;p&gt;Pulls: 19.8M&lt;/p&gt;&lt;p&gt;Tags: 3&lt;/p&gt;</content>
  </entry>
  <entry>
    <title>gemma</title>
    <id>https://ollama.com/library/gemma</id>
    <link href="https://ollama.com/library/gemma"/>
    <summary>Gemma is a family of lightweight, state-of-the-art open models built by Google DeepMind. Updated to version 1.1</summary>
    <updated>2024-04-09T13:37:00</updated>
    <category term="2b"/>
    <category term="7b"/>
    <content type="html">&lt;p&gt;Gemma is a family of lightweight, state-of-the-art open models built by Google DeepMind. Updated to version 1.1&lt;/p&gt;&lt;p&gt;Pulls: 4.4M&lt;/p&gt;&lt;p&gt;Tags: 102&lt;/p&gt;</content>
  </entry>
  <entry>
    <title>stablelm2</title>
    <id>https://ollama.com/library/stablelm2</id>
    <link href="https://ollama.com/library/stablelm2"/>
    <summary>Stable LM 2 is a state-of-the-art 1.6B and 12B parameter language model trained on multilingual data in English, Spanish, German, Italian, French, Portuguese, and Dutch.</summary>
    <updated>2024-05-07T02:11:00</updated>
    <category term="1.6b"/>
    <category term="12b"/>
    <content type="html">&lt;p&gt;Stable LM 2 is a state-of-the-art 1.6B and 12B parameter language model trained on multilingual data in English, Spanish, German, Italian, French, Portuguese, and Dutch.&lt;/p&gt;&lt;p&gt;Pulls: 109.1K&lt;/p&gt;&lt;p&gt;Tags: 84&lt;/p&gt;</content>
  </entry>
  <entry>
    <title>duckdb-nsql</title>
    <id>https://ollama.com/library/duckdb-nsql</id>
    <link href="https://ollama.com/library/duckdb-nsql"/>
    <summary>7B parameter text-to-SQL model made by MotherDuck and Numbers Station.</summary>
    <updated>2024-01-29T23:52:00</updated>
    <category term="7b"/>
    <content type="html">&lt;p&gt;7B parameter text-to-SQL model made by MotherDuck and Numbers Station.&lt;/p&gt;&lt;p&gt;Pulls: 31K&lt;/p&gt;&lt;p&gt;Tags: 17&lt;/p&gt;</content>
  </entry>
  <entry>
    <title>qwen</title>
    <id>https://ollama.com/library/qwen</id>
    <link href="https://ollama.com/library/qwen"/>
    <summary>Qwen 1.5 is a series of large language models by Alibaba Cloud spanning from 0.5B to 110B parameters</summary>
    <updated>2024-04-26T20:31:00</updated>
    <category term="0.5b"/>
    <category term="1.8b"/>
    <category term="4b"/>
    <category term="7b"/>
    <category term="14b"/>
    <category term="32b"/>
    <category term="72b"/>
    <category term="110b"/>
    <content type="html">&lt;p&gt;Qwen 1.5 is a series of large language models by Alibaba Cloud spanning from 0.5B to 110B parameters&lt;/p&gt;&lt;p&gt;Pulls: 4.5M&lt;/p&gt;&lt;p&gt;Tags: 379&lt;/p&gt;</content>
  </entry>
  <entry>
    <title>tinydolphin</title>
    <id>https://ollama.com/library/tinydolphin</id>
    <link href="https://ollama.com/library/tinydolphin"/>
    <summary>An experimental 1.1B parameter model trained on the new Dolphin 2.8 dataset by Eric Hartford and based on TinyLlama.</summary>
    <updated>2024-01-30T09:39:00</updated>
    <category term="1.1b"/>
    <content type="html">&lt;p&gt;An experimental 1.1B parameter model trained on the new Dolphin 2.8 dataset by Eric Hartford and based on TinyLlama.&lt;/p&gt;&lt;p&gt;Pulls: 121.8K&lt;/p&gt;&lt;p&gt;Tags: 18&lt;/p&gt;</content>
  </entry>
  <entry>
    <title>stable-code</title>
    <id>https://ollama.com/library/stable-code</id>
    <link href="https://ollama.com/library/stable-code"/>
    <summary>Stable Code 3B is a coding model with instruct and code completion variants on par with models such as Code Llama 7B that are 2.5x larger.</summary>
    <updated>2024-03-26T02:45:00</updated>
    <category term="3b"/>
    <content type="html">&lt;p&gt;Stable Code 3B is a coding model with instruct and code completion variants on par with models such as Code Llama 7B that are 2.5x larger.&lt;/p&gt;&lt;p&gt;Pulls: 123.7K&lt;/p&gt;&lt;p&gt;Tags: 36&lt;/p&gt;</content>
  </entry>
  <entry>
    <title>nous-hermes2-mixtral</title>
    <id>https://ollama.com/library/nous-hermes2-mixtral</id>
    <link href="https://ollama.com/library/nous-hermes2-mixtral"/>
    <summary>The Nous Hermes 2 model from Nous Research, now trained over Mixtral.</summary>
    <updated>2024-12-20T01:05:00</updated>
    <category term="8x7b"/>
    <content type="html">&lt;p&gt;The Nous Hermes 2 model from Nous Research, now trained over Mixtral.&lt;/p&gt;&lt;p&gt;Pulls: 38.7K&lt;/p&gt;&lt;p&gt;Tags: 18&lt;/p&gt;</content>
  </entry>
  <entry>
    <title>megadolphin</title>
    <id>https://ollama.com/library/megadolphin</id>
    <link href="https://ollama.com/library/megadolphin"/>
    <summary>MegaDolphin-2.2-120b is a transformation of Dolphin-2.2-70b created by interleaving the model with itself.</summary>
    <updated>2024-01-11T01:25:00</updated>
    <category term="120b"/>
    <content type="html">&lt;p&gt;MegaDolphin-2.2-120b is a transformation of Dolphin-2.2-70b created by interleaving the model with itself.&lt;/p&gt;&lt;p&gt;Pulls: 25.4K&lt;/p&gt;&lt;p&gt;Tags: 19&lt;/p&gt;</content>
  </entry>
  <entry>
    <title>llama-pro</title>
    <id>https://ollama.com/library/llama-pro</id>
    <link href="https://ollama.com/library/llama-pro"/>
    <summary>An expansion of Llama 2 that specializes in integrating both general language understanding and domain-specific knowledge, particularly in programming and mathematics.</summary>
    <updated>2024-01-09T03:30:00</updated>
    <content type="html">&lt;p&gt;An expansion of Llama 2 that specializes in integrating both general language understanding and domain-specific knowledge, particularly in programming and mathematics.&lt;/p&gt;&lt;p&gt;Pulls: 46.1K&lt;/p&gt;&lt;p&gt;Tags: 33&lt;/p&gt;</content>
  </entry>
  <entry>
    <title>tinyllama</title>
    <id>https://ollama.com/library/tinyllama</id>
    <link href="https://ollama.com/library/tinyllama"/>
    <summary>The TinyLlama project is an open endeavor to train a compact 1.1B Llama model on 3 trillion tokens.</summary>
    <updated>2024-01-01T02:44:00</updated>
    <category term="1.1b"/>
    <content type="html">&lt;p&gt;The TinyLlama project is an open endeavor to train a compact 1.1B Llama model on 3 trillion tokens.&lt;/p&gt;&lt;p&gt;Pulls: 1.4M&lt;/p&gt;&lt;p&gt;Tags: 36&lt;/p&gt;</content>
  </entry>
  <entry>
    <title>openhermes</title>
    <id>https://ollama.com/library/openhermes</id>
    <link href="https://ollama.com/library/openhermes"/>
    <summary>OpenHermes 2.5 is a 7B model fine-tuned by Teknium on Mistral with fully open datasets.</summary>
    <updated>2023-12-29T21:45:00</updated>
    <content type="html">&lt;p&gt;OpenHermes 2.5 is a 7B model fine-tuned by Teknium on Mistral with fully open datasets.&lt;/p&gt;&lt;p&gt;Pulls: 124.6K&lt;/p&gt;&lt;p&gt;Tags: 35&lt;/p&gt;</content>
  </entry>
  <entry>
    <title>notux</title>
    <id>https://ollama.com/library/notux</id>
    <link href="https://ollama.com/library/notux"/>
    <summary>A top-performing mixture of experts model, fine-tuned with high-quality data.</summary>
    <updated>2023-12-29T03:35:00</updated>
    <category term="8x7b"/>
    <content type="html">&lt;p&gt;A top-performing mixture of experts model, fine-tuned with high-quality data.&lt;/p&gt;&lt;p&gt;Pulls: 24.5K&lt;/p&gt;&lt;p&gt;Tags: 18&lt;/p&gt;</content>
  </entry>
  <entry>
    <title>notus</title>
    <id>https://ollama.com/library/notus</id>
    <link href="https://ollama.com/library/notus"/>
    <summary>A 7B chat model fine-tuned with high-quality data and based on Zephyr.</summary>
    <updated>2023-12-29T22:06:00</updated>
    <category term="7b"/>
    <content type="html">&lt;p&gt;A 7B chat model fine-tuned with high-quality data and based on Zephyr.&lt;/p&gt;&lt;p&gt;Pulls: 23.9K&lt;/p&gt;&lt;p&gt;Tags: 18&lt;/p&gt;</content>
  </entry>
  <entry>
    <title>dolphin-mistral</title>
    <id>https://ollama.com/library/dolphin-mistral</id>
    <link href="https://ollama.com/library/dolphin-mistral"/>
    <summary>The uncensored Dolphin model based on Mistral that excels at coding tasks. Updated to version 2.8.</summary>
    <updated>2024-03-31T23:34:00</updated>
    <category term="7b"/>
    <content type="html">&lt;p&gt;The uncensored Dolphin model based on Mistral that excels at coding tasks. Updated to version 2.8.&lt;/p&gt;&lt;p&gt;Pulls: 329K&lt;/p&gt;&lt;p&gt;Tags: 120&lt;/p&gt;</content>
  </entry>
  <entry>
    <title>nous-hermes2</title>
    <id>https://ollama.com/library/nous-hermes2</id>
    <link href="https://ollama.com/library/nous-hermes2"/>
    <summary>The powerful family of models by Nous Research that excels at scientific discussion and coding tasks.</summary>
    <updated>2024-01-02T14:26:00</updated>
    <category term="10.7b"/>
    <category term="34b"/>
    <content type="html">&lt;p&gt;The powerful family of models by Nous Research that excels at scientific discussion and coding tasks.&lt;/p&gt;&lt;p&gt;Pulls: 122.4K&lt;/p&gt;&lt;p&gt;Tags: 33&lt;/p&gt;</content>
  </entry>
  <entry>
    <title>dolphin-phi</title>
    <id>https://ollama.com/library/dolphin-phi</id>
    <link href="https://ollama.com/library/dolphin-phi"/>
    <summary>2.7B uncensored Dolphin model by Eric Hartford, based on the Phi language model by Microsoft Research.</summary>
    <updated>2023-12-24T16:37:00</updated>
    <category term="2.7b"/>
    <content type="html">&lt;p&gt;2.7B uncensored Dolphin model by Eric Hartford, based on the Phi language model by Microsoft Research.&lt;/p&gt;&lt;p&gt;Pulls: 67.3K&lt;/p&gt;&lt;p&gt;Tags: 15&lt;/p&gt;</content>
  </entry>
  <entry>
    <title>phi</title>
    <id>https://ollama.com/library/phi</id>
    <link href="https://ollama.com/library/phi"/>
    <summary>Phi-2: a 2.7B language model by Microsoft Research that demonstrates outstanding reasoning and language understanding capabilities.</summary>
    <updated>2024-01-06T17:10:00</updated>
    <category term="2.7b"/>
    <content type="html">&lt;p&gt;Phi-2: a 2.7B language model by Microsoft Research that demonstrates outstanding reasoning and language understanding capabilities.&lt;/p&gt;&lt;p&gt;Pulls: 500.9K&lt;/p&gt;&lt;p&gt;Tags: 18&lt;/p&gt;</content>
  </entry>
  <entry>
    <title>solar</title>
    <id>https://ollama.com/library/solar</id>
    <link href="https://ollama.com/library/solar"/>
    <summary>A compact, yet powerful 10.7B large language model designed for single-turn conversation.</summary>
    <updated>2023-12-18T03:00:00</updated>
    <category term="10.7b"/>
    <content type="html">&lt;p&gt;A compact, yet powerful 10.7B large language model designed for single-turn conversation.&lt;/p&gt;&lt;p&gt;Pulls: 79.6K&lt;/p&gt;&lt;p&gt;Tags: 32&lt;/p&gt;</content>
  </entry>
  <entry>
    <title>dolphin-mixtral</title>
    <id>https://ollama.com/library/dolphin-mixtral</id>
    <link href="https://ollama.com/library/dolphin-mixtral"/>
    <summary>Uncensored, 8x7b and 8x22b fine-tuned models based on the Mixtral mixture of experts models that excels at coding tasks. Created by Eric Hartford.</summary>
    <updated>2024-12-20T02:11:00</updated>
    <category term="8x7b"/>
    <category term="8x22b"/>
    <content type="html">&lt;p&gt;Uncensored, 8x7b and 8x22b fine-tuned models based on the Mixtral mixture of experts models that excels at coding tasks. Created by Eric Hartford.&lt;/p&gt;&lt;p&gt;Pulls: 527.5K&lt;/p&gt;&lt;p&gt;Tags: 70&lt;/p&gt;</content>
  </entry>
  <entry>
    <title>mixtral</title>
    <id>https://ollama.com/library/mixtral</id>
    <link href="https://ollama.com/library/mixtral"/>
    <summary>A set of Mixture of Experts (MoE) model with open weights by Mistral AI in 8x7b and 8x22b parameter sizes.</summary>
    <updated>2024-12-20T08:27:00</updated>
    <category term="8x7b"/>
    <category term="8x22b"/>
    <category term="tools"/>
    <content type="html">&lt;p&gt;A set of Mixture of Experts (MoE) model with open weights by Mistral AI in 8x7b and 8x22b parameter sizes.&lt;/p&gt;&lt;p&gt;Pulls: 591.9K&lt;/p&gt;&lt;p&gt;Tags: 70&lt;/p&gt;</content>
  </entry>
  <entry>
    <title>bakllava</title>
    <id>https://ollama.com/library/bakllava</id>
    <link href="https://ollama.com/library/bakllava"/>
    <summary>BakLLaVA is a multimodal model consisting of the Mistral 7B base model augmented with the LLaVA  architecture.</summary>
    <updated>2023-12-13T19:04:00</updated>
    <category term="7b"/>
    <category term="vision"/>
    <content type="html">&lt;p&gt;BakLLaVA is a multimodal model consisting of the Mistral 7B base model augmented with the LLaVA  architecture.&lt;/p&gt;&lt;p&gt;Pulls: 111K&lt;/p&gt;&lt;p&gt;Tags: 17&lt;/p&gt;</content>
  </entry>
  <entry>
    <title>llava</title>
    <id>https://ollama.com/library/llava</id>
    <link href="https://ollama.com/library/llava"/>
    <summary>üåã LLaVA is a novel end-to-end trained large multimodal model that combines a vision encoder and Vicuna for general-purpose visual and language understanding. Updated to version 1.6.</summary>
    <updated>2024-02-01T08:41:00</updated>
    <category term="7b"/>
    <category term="13b"/>
    <category term="34b"/>
    <category term="vision"/>
    <content type="html">&lt;p&gt;üåã LLaVA is a novel end-to-end trained large multimodal model that combines a vision encoder and Vicuna for general-purpose visual and language understanding. Updated to version 1.6.&lt;/p&gt;&lt;p&gt;Pulls: 4.3M&lt;/p&gt;&lt;p&gt;Tags: 98&lt;/p&gt;</content>
  </entry>
  <entry>
    <title>stablelm-zephyr</title>
    <id>https://ollama.com/library/stablelm-zephyr</id>
    <link href="https://ollama.com/library/stablelm-zephyr"/>
    <summary>A lightweight chat model allowing accurate, and responsive output without requiring high-end hardware.</summary>
    <updated>2023-12-27T20:51:00</updated>
    <category term="3b"/>
    <content type="html">&lt;p&gt;A lightweight chat model allowing accurate, and responsive output without requiring high-end hardware.&lt;/p&gt;&lt;p&gt;Pulls: 32.4K&lt;/p&gt;&lt;p&gt;Tags: 17&lt;/p&gt;</content>
  </entry>
  <entry>
    <title>magicoder</title>
    <id>https://ollama.com/library/magicoder</id>
    <link href="https://ollama.com/library/magicoder"/>
    <summary>üé© Magicoder is a family of 7B parameter models trained on 75K synthetic instruction data using OSS-Instruct, a novel approach to enlightening LLMs with open-source code snippets.</summary>
    <updated>2023-12-05T23:59:00</updated>
    <category term="7b"/>
    <content type="html">&lt;p&gt;üé© Magicoder is a family of 7B parameter models trained on 75K synthetic instruction data using OSS-Instruct, a novel approach to enlightening LLMs with open-source code snippets.&lt;/p&gt;&lt;p&gt;Pulls: 32.4K&lt;/p&gt;&lt;p&gt;Tags: 18&lt;/p&gt;</content>
  </entry>
  <entry>
    <title>deepseek-llm</title>
    <id>https://ollama.com/library/deepseek-llm</id>
    <link href="https://ollama.com/library/deepseek-llm"/>
    <summary>An advanced language model crafted with 2 trillion bilingual tokens.</summary>
    <updated>2023-12-11T19:53:00</updated>
    <category term="7b"/>
    <category term="67b"/>
    <content type="html">&lt;p&gt;An advanced language model crafted with 2 trillion bilingual tokens.&lt;/p&gt;&lt;p&gt;Pulls: 134.5K&lt;/p&gt;&lt;p&gt;Tags: 64&lt;/p&gt;</content>
  </entry>
  <entry>
    <title>meditron</title>
    <id>https://ollama.com/library/meditron</id>
    <link href="https://ollama.com/library/meditron"/>
    <summary>Open-source medical large language model adapted from Llama 2 to the medical domain.</summary>
    <updated>2023-12-05T18:31:00</updated>
    <category term="7b"/>
    <category term="70b"/>
    <content type="html">&lt;p&gt;Open-source medical large language model adapted from Llama 2 to the medical domain.&lt;/p&gt;&lt;p&gt;Pulls: 48.6K&lt;/p&gt;&lt;p&gt;Tags: 22&lt;/p&gt;</content>
  </entry>
  <entry>
    <title>starling-lm</title>
    <id>https://ollama.com/library/starling-lm</id>
    <link href="https://ollama.com/library/starling-lm"/>
    <summary>Starling is a large language model trained by reinforcement learning from AI feedback focused on improving chatbot helpfulness.</summary>
    <updated>2024-04-02T23:45:00</updated>
    <category term="7b"/>
    <content type="html">&lt;p&gt;Starling is a large language model trained by reinforcement learning from AI feedback focused on improving chatbot helpfulness.&lt;/p&gt;&lt;p&gt;Pulls: 81.8K&lt;/p&gt;&lt;p&gt;Tags: 36&lt;/p&gt;</content>
  </entry>
  <entry>
    <title>orca2</title>
    <id>https://ollama.com/library/orca2</id>
    <link href="https://ollama.com/library/orca2"/>
    <summary>Orca 2 is built by Microsoft research, and are a fine-tuned version of Meta's Llama 2 models.  The model is designed to excel particularly in reasoning.</summary>
    <updated>2023-11-22T22:13:00</updated>
    <category term="7b"/>
    <category term="13b"/>
    <content type="html">&lt;p&gt;Orca 2 is built by Microsoft research, and are a fine-tuned version of Meta's Llama 2 models.  The model is designed to excel particularly in reasoning.&lt;/p&gt;&lt;p&gt;Pulls: 64.2K&lt;/p&gt;&lt;p&gt;Tags: 33&lt;/p&gt;</content>
  </entry>
  <entry>
    <title>deepseek-coder</title>
    <id>https://ollama.com/library/deepseek-coder</id>
    <link href="https://ollama.com/library/deepseek-coder"/>
    <summary>DeepSeek Coder is a capable coding model trained on two trillion code and natural language tokens.</summary>
    <updated>2023-12-28T01:05:00</updated>
    <category term="1.3b"/>
    <category term="6.7b"/>
    <category term="33b"/>
    <content type="html">&lt;p&gt;DeepSeek Coder is a capable coding model trained on two trillion code and natural language tokens.&lt;/p&gt;&lt;p&gt;Pulls: 616.4K&lt;/p&gt;&lt;p&gt;Tags: 102&lt;/p&gt;</content>
  </entry>
  <entry>
    <title>alfred</title>
    <id>https://ollama.com/library/alfred</id>
    <link href="https://ollama.com/library/alfred"/>
    <summary>A robust conversational model designed to be used for both chat and instruct use cases.</summary>
    <updated>2023-11-19T13:58:00</updated>
    <category term="40b"/>
    <content type="html">&lt;p&gt;A robust conversational model designed to be used for both chat and instruct use cases.&lt;/p&gt;&lt;p&gt;Pulls: 16.1K&lt;/p&gt;&lt;p&gt;Tags: 7&lt;/p&gt;</content>
  </entry>
  <entry>
    <title>goliath</title>
    <id>https://ollama.com/library/goliath</id>
    <link href="https://ollama.com/library/goliath"/>
    <summary>A language model created by combining two fine-tuned Llama 2 70B models into one.</summary>
    <updated>2023-11-17T21:54:00</updated>
    <content type="html">&lt;p&gt;A language model created by combining two fine-tuned Llama 2 70B models into one.&lt;/p&gt;&lt;p&gt;Pulls: 23.1K&lt;/p&gt;&lt;p&gt;Tags: 16&lt;/p&gt;</content>
  </entry>
  <entry>
    <title>neural-chat</title>
    <id>https://ollama.com/library/neural-chat</id>
    <link href="https://ollama.com/library/neural-chat"/>
    <summary>A fine-tuned model based on Mistral with good coverage of domain and language.</summary>
    <updated>2023-12-20T17:02:00</updated>
    <category term="7b"/>
    <content type="html">&lt;p&gt;A fine-tuned model based on Mistral with good coverage of domain and language.&lt;/p&gt;&lt;p&gt;Pulls: 105.6K&lt;/p&gt;&lt;p&gt;Tags: 50&lt;/p&gt;</content>
  </entry>
  <entry>
    <title>openchat</title>
    <id>https://ollama.com/library/openchat</id>
    <link href="https://ollama.com/library/openchat"/>
    <summary>A family of open-source models trained on a wide variety of data, surpassing ChatGPT on various benchmarks. Updated to version 3.5-0106.</summary>
    <updated>2024-01-10T21:45:00</updated>
    <category term="7b"/>
    <content type="html">&lt;p&gt;A family of open-source models trained on a wide variety of data, surpassing ChatGPT on various benchmarks. Updated to version 3.5-0106.&lt;/p&gt;&lt;p&gt;Pulls: 147.3K&lt;/p&gt;&lt;p&gt;Tags: 50&lt;/p&gt;</content>
  </entry>
  <entry>
    <title>yi</title>
    <id>https://ollama.com/library/yi</id>
    <link href="https://ollama.com/library/yi"/>
    <summary>Yi 1.5 is a high-performing, bilingual language model.</summary>
    <updated>2024-05-13T08:22:00</updated>
    <category term="6b"/>
    <category term="9b"/>
    <category term="34b"/>
    <content type="html">&lt;p&gt;Yi 1.5 is a high-performing, bilingual language model.&lt;/p&gt;&lt;p&gt;Pulls: 268.9K&lt;/p&gt;&lt;p&gt;Tags: 174&lt;/p&gt;</content>
  </entry>
  <entry>
    <title>yarn-mistral</title>
    <id>https://ollama.com/library/yarn-mistral</id>
    <link href="https://ollama.com/library/yarn-mistral"/>
    <summary>An extension of Mistral to support context windows of 64K or 128K.</summary>
    <updated>2023-11-04T04:20:00</updated>
    <category term="7b"/>
    <content type="html">&lt;p&gt;An extension of Mistral to support context windows of 64K or 128K.&lt;/p&gt;&lt;p&gt;Pulls: 45.5K&lt;/p&gt;&lt;p&gt;Tags: 33&lt;/p&gt;</content>
  </entry>
  <entry>
    <title>yarn-llama2</title>
    <id>https://ollama.com/library/yarn-llama2</id>
    <link href="https://ollama.com/library/yarn-llama2"/>
    <summary>An extension of Llama 2 that supports a context of up to 128k tokens.</summary>
    <updated>2023-11-04T22:50:00</updated>
    <category term="7b"/>
    <category term="13b"/>
    <content type="html">&lt;p&gt;An extension of Llama 2 that supports a context of up to 128k tokens.&lt;/p&gt;&lt;p&gt;Pulls: 79.3K&lt;/p&gt;&lt;p&gt;Tags: 67&lt;/p&gt;</content>
  </entry>
  <entry>
    <title>xwinlm</title>
    <id>https://ollama.com/library/xwinlm</id>
    <link href="https://ollama.com/library/xwinlm"/>
    <summary>Conversational model based on Llama 2 that performs competitively on various benchmarks.</summary>
    <updated>2023-11-04T06:11:00</updated>
    <category term="7b"/>
    <category term="13b"/>
    <content type="html">&lt;p&gt;Conversational model based on Llama 2 that performs competitively on various benchmarks.&lt;/p&gt;&lt;p&gt;Pulls: 84.8K&lt;/p&gt;&lt;p&gt;Tags: 80&lt;/p&gt;</content>
  </entry>
  <entry>
    <title>mistrallite</title>
    <id>https://ollama.com/library/mistrallite</id>
    <link href="https://ollama.com/library/mistrallite"/>
    <summary>MistralLite is a fine-tuned model based on Mistral with enhanced capabilities of processing long contexts.</summary>
    <updated>2023-11-02T22:27:00</updated>
    <category term="7b"/>
    <content type="html">&lt;p&gt;MistralLite is a fine-tuned model based on Mistral with enhanced capabilities of processing long contexts.&lt;/p&gt;&lt;p&gt;Pulls: 30.5K&lt;/p&gt;&lt;p&gt;Tags: 17&lt;/p&gt;</content>
  </entry>
  <entry>
    <title>codebooga</title>
    <id>https://ollama.com/library/codebooga</id>
    <link href="https://ollama.com/library/codebooga"/>
    <summary>A high-performing code instruct model created by merging two existing code models.</summary>
    <updated>2023-10-29T10:48:00</updated>
    <category term="34b"/>
    <content type="html">&lt;p&gt;A high-performing code instruct model created by merging two existing code models.&lt;/p&gt;&lt;p&gt;Pulls: 31.6K&lt;/p&gt;&lt;p&gt;Tags: 16&lt;/p&gt;</content>
  </entry>
  <entry>
    <title>mistral-openorca</title>
    <id>https://ollama.com/library/mistral-openorca</id>
    <link href="https://ollama.com/library/mistral-openorca"/>
    <summary>Mistral OpenOrca is a 7 billion parameter model, fine-tuned on top of the Mistral 7B model using the OpenOrca dataset.</summary>
    <updated>2023-10-11T22:00:00</updated>
    <category term="7b"/>
    <content type="html">&lt;p&gt;Mistral OpenOrca is a 7 billion parameter model, fine-tuned on top of the Mistral 7B model using the OpenOrca dataset.&lt;/p&gt;&lt;p&gt;Pulls: 167.6K&lt;/p&gt;&lt;p&gt;Tags: 17&lt;/p&gt;</content>
  </entry>
  <entry>
    <title>zephyr</title>
    <id>https://ollama.com/library/zephyr</id>
    <link href="https://ollama.com/library/zephyr"/>
    <summary>Zephyr is a series of fine-tuned versions of the Mistral and Mixtral models that are trained to act as helpful assistants.</summary>
    <updated>2024-04-16T00:10:00</updated>
    <category term="7b"/>
    <category term="141b"/>
    <content type="html">&lt;p&gt;Zephyr is a series of fine-tuned versions of the Mistral and Mixtral models that are trained to act as helpful assistants.&lt;/p&gt;&lt;p&gt;Pulls: 239.6K&lt;/p&gt;&lt;p&gt;Tags: 40&lt;/p&gt;</content>
  </entry>
  <entry>
    <title>nexusraven</title>
    <id>https://ollama.com/library/nexusraven</id>
    <link href="https://ollama.com/library/nexusraven"/>
    <summary>Nexus Raven is a 13B instruction tuned model for function calling tasks.</summary>
    <updated>2024-01-17T19:53:00</updated>
    <category term="13b"/>
    <content type="html">&lt;p&gt;Nexus Raven is a 13B instruction tuned model for function calling tasks.&lt;/p&gt;&lt;p&gt;Pulls: 41.9K&lt;/p&gt;&lt;p&gt;Tags: 32&lt;/p&gt;</content>
  </entry>
  <entry>
    <title>samantha-mistral</title>
    <id>https://ollama.com/library/samantha-mistral</id>
    <link href="https://ollama.com/library/samantha-mistral"/>
    <summary>A companion assistant trained in philosophy, psychology, and personal relationships. Based on Mistral.</summary>
    <updated>2023-10-15T16:10:00</updated>
    <category term="7b"/>
    <content type="html">&lt;p&gt;A companion assistant trained in philosophy, psychology, and personal relationships. Based on Mistral.&lt;/p&gt;&lt;p&gt;Pulls: 90.7K&lt;/p&gt;&lt;p&gt;Tags: 49&lt;/p&gt;</content>
  </entry>
  <entry>
    <title>starcoder</title>
    <id>https://ollama.com/library/starcoder</id>
    <link href="https://ollama.com/library/starcoder"/>
    <summary>StarCoder is a code generation model trained on 80+ programming languages.</summary>
    <updated>2023-10-24T09:19:00</updated>
    <category term="1b"/>
    <category term="3b"/>
    <category term="7b"/>
    <category term="15b"/>
    <content type="html">&lt;p&gt;StarCoder is a code generation model trained on 80+ programming languages.&lt;/p&gt;&lt;p&gt;Pulls: 189.3K&lt;/p&gt;&lt;p&gt;Tags: 100&lt;/p&gt;</content>
  </entry>
  <entry>
    <title>sqlcoder</title>
    <id>https://ollama.com/library/sqlcoder</id>
    <link href="https://ollama.com/library/sqlcoder"/>
    <summary>SQLCoder is a code completion model fined-tuned on StarCoder for SQL generation tasks</summary>
    <updated>2024-02-01T04:26:00</updated>
    <category term="7b"/>
    <category term="15b"/>
    <content type="html">&lt;p&gt;SQLCoder is a code completion model fined-tuned on StarCoder for SQL generation tasks&lt;/p&gt;&lt;p&gt;Pulls: 96.7K&lt;/p&gt;&lt;p&gt;Tags: 48&lt;/p&gt;</content>
  </entry>
  <entry>
    <title>mistral</title>
    <id>https://ollama.com/library/mistral</id>
    <link href="https://ollama.com/library/mistral"/>
    <summary>The 7B model released by Mistral AI, updated to version 0.3.</summary>
    <updated>2024-07-22T19:55:00</updated>
    <category term="7b"/>
    <category term="tools"/>
    <content type="html">&lt;p&gt;The 7B model released by Mistral AI, updated to version 0.3.&lt;/p&gt;&lt;p&gt;Pulls: 10.7M&lt;/p&gt;&lt;p&gt;Tags: 84&lt;/p&gt;</content>
  </entry>
  <entry>
    <title>falcon</title>
    <id>https://ollama.com/library/falcon</id>
    <link href="https://ollama.com/library/falcon"/>
    <summary>A large language model built by the Technology Innovation Institute (TII) for use in summarization, text generation, and chat bots.</summary>
    <updated>2023-10-26T01:41:00</updated>
    <category term="7b"/>
    <category term="40b"/>
    <category term="180b"/>
    <content type="html">&lt;p&gt;A large language model built by the Technology Innovation Institute (TII) for use in summarization, text generation, and chat bots.&lt;/p&gt;&lt;p&gt;Pulls: 70.9K&lt;/p&gt;&lt;p&gt;Tags: 38&lt;/p&gt;</content>
  </entry>
  <entry>
    <title>wizardcoder</title>
    <id>https://ollama.com/library/wizardcoder</id>
    <link href="https://ollama.com/library/wizardcoder"/>
    <summary>State-of-the-art code generation model</summary>
    <updated>2024-01-05T05:09:00</updated>
    <category term="33b"/>
    <content type="html">&lt;p&gt;State-of-the-art code generation model&lt;/p&gt;&lt;p&gt;Pulls: 118.1K&lt;/p&gt;&lt;p&gt;Tags: 67&lt;/p&gt;</content>
  </entry>
  <entry>
    <title>phind-codellama</title>
    <id>https://ollama.com/library/phind-codellama</id>
    <link href="https://ollama.com/library/phind-codellama"/>
    <summary>Code generation model based on Code Llama.</summary>
    <updated>2023-12-28T02:53:00</updated>
    <category term="34b"/>
    <content type="html">&lt;p&gt;Code generation model based on Code Llama.&lt;/p&gt;&lt;p&gt;Pulls: 82K&lt;/p&gt;&lt;p&gt;Tags: 49&lt;/p&gt;</content>
  </entry>
  <entry>
    <title>codellama</title>
    <id>https://ollama.com/library/codellama</id>
    <link href="https://ollama.com/library/codellama"/>
    <summary>A large language model that can use text prompts to generate and discuss code.</summary>
    <updated>2024-07-18T17:31:00</updated>
    <category term="7b"/>
    <category term="13b"/>
    <category term="34b"/>
    <category term="70b"/>
    <content type="html">&lt;p&gt;A large language model that can use text prompts to generate and discuss code.&lt;/p&gt;&lt;p&gt;Pulls: 1.9M&lt;/p&gt;&lt;p&gt;Tags: 199&lt;/p&gt;</content>
  </entry>
  <entry>
    <title>wizardlm</title>
    <id>https://ollama.com/library/wizardlm</id>
    <link href="https://ollama.com/library/wizardlm"/>
    <summary>General use model based on Llama 2.</summary>
    <updated>2023-10-30T02:54:00</updated>
    <content type="html">&lt;p&gt;General use model based on Llama 2.&lt;/p&gt;&lt;p&gt;Pulls: 76.1K&lt;/p&gt;&lt;p&gt;Tags: 73&lt;/p&gt;</content>
  </entry>
  <entry>
    <title>wizardlm-uncensored</title>
    <id>https://ollama.com/library/wizardlm-uncensored</id>
    <link href="https://ollama.com/library/wizardlm-uncensored"/>
    <summary>Uncensored version of Wizard LM model</summary>
    <updated>2023-10-28T15:29:00</updated>
    <category term="13b"/>
    <content type="html">&lt;p&gt;Uncensored version of Wizard LM model&lt;/p&gt;&lt;p&gt;Pulls: 61.6K&lt;/p&gt;&lt;p&gt;Tags: 18&lt;/p&gt;</content>
  </entry>
  <entry>
    <title>wizard-vicuna</title>
    <id>https://ollama.com/library/wizard-vicuna</id>
    <link href="https://ollama.com/library/wizard-vicuna"/>
    <summary>Wizard Vicuna is a 13B parameter model based on Llama 2 trained by MelodysDreamj.</summary>
    <updated>2023-10-28T19:41:00</updated>
    <category term="13b"/>
    <content type="html">&lt;p&gt;Wizard Vicuna is a 13B parameter model based on Llama 2 trained by MelodysDreamj.&lt;/p&gt;&lt;p&gt;Pulls: 29.8K&lt;/p&gt;&lt;p&gt;Tags: 17&lt;/p&gt;</content>
  </entry>
  <entry>
    <title>wizard-vicuna-uncensored</title>
    <id>https://ollama.com/library/wizard-vicuna-uncensored</id>
    <link href="https://ollama.com/library/wizard-vicuna-uncensored"/>
    <summary>Wizard Vicuna Uncensored is a 7B, 13B, and 30B parameter model based on Llama 2 uncensored by Eric Hartford.</summary>
    <updated>2023-10-28T17:20:00</updated>
    <category term="7b"/>
    <category term="13b"/>
    <category term="30b"/>
    <content type="html">&lt;p&gt;Wizard Vicuna Uncensored is a 7B, 13B, and 30B parameter model based on Llama 2 uncensored by Eric Hartford.&lt;/p&gt;&lt;p&gt;Pulls: 188.5K&lt;/p&gt;&lt;p&gt;Tags: 49&lt;/p&gt;</content>
  </entry>
  <entry>
    <title>wizard-math</title>
    <id>https://ollama.com/library/wizard-math</id>
    <link href="https://ollama.com/library/wizard-math"/>
    <summary>Model focused on math and logic problems</summary>
    <updated>2023-12-19T16:02:00</updated>
    <category term="7b"/>
    <category term="13b"/>
    <category term="70b"/>
    <content type="html">&lt;p&gt;Model focused on math and logic problems&lt;/p&gt;&lt;p&gt;Pulls: 101.7K&lt;/p&gt;&lt;p&gt;Tags: 64&lt;/p&gt;</content>
  </entry>
  <entry>
    <title>vicuna</title>
    <id>https://ollama.com/library/vicuna</id>
    <link href="https://ollama.com/library/vicuna"/>
    <summary>General use chat model based on Llama and Llama 2 with 2K to 16K context sizes.</summary>
    <updated>2023-10-28T16:30:00</updated>
    <category term="7b"/>
    <category term="13b"/>
    <category term="33b"/>
    <content type="html">&lt;p&gt;General use chat model based on Llama and Llama 2 with 2K to 16K context sizes.&lt;/p&gt;&lt;p&gt;Pulls: 177.2K&lt;/p&gt;&lt;p&gt;Tags: 111&lt;/p&gt;</content>
  </entry>
  <entry>
    <title>stable-beluga</title>
    <id>https://ollama.com/library/stable-beluga</id>
    <link href="https://ollama.com/library/stable-beluga"/>
    <summary>Llama 2 based model fine tuned on an Orca-style dataset. Originally called Free Willy.</summary>
    <updated>2023-10-29T21:57:00</updated>
    <category term="7b"/>
    <category term="13b"/>
    <category term="70b"/>
    <content type="html">&lt;p&gt;Llama 2 based model fine tuned on an Orca-style dataset. Originally called Free Willy.&lt;/p&gt;&lt;p&gt;Pulls: 59.3K&lt;/p&gt;&lt;p&gt;Tags: 49&lt;/p&gt;</content>
  </entry>
  <entry>
    <title>orca-mini</title>
    <id>https://ollama.com/library/orca-mini</id>
    <link href="https://ollama.com/library/orca-mini"/>
    <summary>A general-purpose model ranging from 3 billion parameters to 70 billion, suitable for entry-level hardware.</summary>
    <updated>2023-10-29T05:30:00</updated>
    <category term="3b"/>
    <category term="7b"/>
    <category term="13b"/>
    <category term="70b"/>
    <content type="html">&lt;p&gt;A general-purpose model ranging from 3 billion parameters to 70 billion, suitable for entry-level hardware.&lt;/p&gt;&lt;p&gt;Pulls: 287.9K&lt;/p&gt;&lt;p&gt;Tags: 119&lt;/p&gt;</content>
  </entry>
  <entry>
    <title>open-orca-platypus2</title>
    <id>https://ollama.com/library/open-orca-platypus2</id>
    <link href="https://ollama.com/library/open-orca-platypus2"/>
    <summary>Merge of the Open Orca OpenChat model and the Garage-bAInd Platypus 2 model. Designed for chat and code generation.</summary>
    <updated>2023-10-29T07:52:00</updated>
    <category term="13b"/>
    <content type="html">&lt;p&gt;Merge of the Open Orca OpenChat model and the Garage-bAInd Platypus 2 model. Designed for chat and code generation.&lt;/p&gt;&lt;p&gt;Pulls: 24K&lt;/p&gt;&lt;p&gt;Tags: 17&lt;/p&gt;</content>
  </entry>
  <entry>
    <title>nous-hermes</title>
    <id>https://ollama.com/library/nous-hermes</id>
    <link href="https://ollama.com/library/nous-hermes"/>
    <summary>General use models based on Llama and Llama 2 from Nous Research.</summary>
    <updated>2023-10-29T11:35:00</updated>
    <category term="7b"/>
    <category term="13b"/>
    <content type="html">&lt;p&gt;General use models based on Llama and Llama 2 from Nous Research.&lt;/p&gt;&lt;p&gt;Pulls: 82.9K&lt;/p&gt;&lt;p&gt;Tags: 63&lt;/p&gt;</content>
  </entry>
  <entry>
    <title>medllama2</title>
    <id>https://ollama.com/library/medllama2</id>
    <link href="https://ollama.com/library/medllama2"/>
    <summary>Fine-tuned Llama 2 model to answer medical questions based on an open source medical dataset.</summary>
    <updated>2023-10-29T06:00:00</updated>
    <category term="7b"/>
    <content type="html">&lt;p&gt;Fine-tuned Llama 2 model to answer medical questions based on an open source medical dataset.&lt;/p&gt;&lt;p&gt;Pulls: 48.3K&lt;/p&gt;&lt;p&gt;Tags: 17&lt;/p&gt;</content>
  </entry>
  <entry>
    <title>llama2</title>
    <id>https://ollama.com/library/llama2</id>
    <link href="https://ollama.com/library/llama2"/>
    <summary>Llama 2 is a collection of foundation language models ranging from 7B to 70B parameters.</summary>
    <updated>2023-12-27T21:21:00</updated>
    <category term="7b"/>
    <category term="13b"/>
    <category term="70b"/>
    <content type="html">&lt;p&gt;Llama 2 is a collection of foundation language models ranging from 7B to 70B parameters.&lt;/p&gt;&lt;p&gt;Pulls: 3.1M&lt;/p&gt;&lt;p&gt;Tags: 102&lt;/p&gt;</content>
  </entry>
  <entry>
    <title>llama2-uncensored</title>
    <id>https://ollama.com/library/llama2-uncensored</id>
    <link href="https://ollama.com/library/llama2-uncensored"/>
    <summary>Uncensored Llama 2 model by George Sung and Jarrad Hope.</summary>
    <updated>2023-10-29T02:31:00</updated>
    <category term="7b"/>
    <category term="70b"/>
    <content type="html">&lt;p&gt;Uncensored Llama 2 model by George Sung and Jarrad Hope.&lt;/p&gt;&lt;p&gt;Pulls: 783.1K&lt;/p&gt;&lt;p&gt;Tags: 34&lt;/p&gt;</content>
  </entry>
  <entry>
    <title>llama2-chinese</title>
    <id>https://ollama.com/library/llama2-chinese</id>
    <link href="https://ollama.com/library/llama2-chinese"/>
    <summary>Llama 2 based model fine tuned to improve Chinese dialogue ability.</summary>
    <updated>2023-10-28T22:17:00</updated>
    <category term="7b"/>
    <category term="13b"/>
    <content type="html">&lt;p&gt;Llama 2 based model fine tuned to improve Chinese dialogue ability.&lt;/p&gt;&lt;p&gt;Pulls: 150.5K&lt;/p&gt;&lt;p&gt;Tags: 35&lt;/p&gt;</content>
  </entry>
  <entry>
    <title>everythinglm</title>
    <id>https://ollama.com/library/everythinglm</id>
    <link href="https://ollama.com/library/everythinglm"/>
    <summary>Uncensored Llama2 based model with support for a 16K context window.</summary>
    <updated>2023-12-27T20:51:00</updated>
    <category term="13b"/>
    <content type="html">&lt;p&gt;Uncensored Llama2 based model with support for a 16K context window.&lt;/p&gt;&lt;p&gt;Pulls: 38.9K&lt;/p&gt;&lt;p&gt;Tags: 18&lt;/p&gt;</content>
  </entry>
  <entry>
    <title>codeup</title>
    <id>https://ollama.com/library/codeup</id>
    <link href="https://ollama.com/library/codeup"/>
    <summary>Great code generation model based on Llama2.</summary>
    <updated>2023-10-29T06:38:00</updated>
    <category term="13b"/>
    <content type="html">&lt;p&gt;Great code generation model based on Llama2.&lt;/p&gt;&lt;p&gt;Pulls: 40K&lt;/p&gt;&lt;p&gt;Tags: 19&lt;/p&gt;</content>
  </entry>
</feed>
